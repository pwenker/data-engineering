{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Data Engineering \u00b6 Data Engineering Project Activity & Issue Tracking Docs News & Updates Purpose of this Project \u00b6 This repository contains my notes for Udacity's Data Engineering Nanodegree The notes are represented in a question-and-answer format to allow for active recall testing If you have completed the degree yourself, you might be interested in testing your knowledge by trying to answer the questions. Regardless of your ability to answer the questions, and the quality of my written answers , the mere exposure and deliberate retrieval practice will help you consolidate your knowledge Have fun!","title":"Home"},{"location":"index.html#data-engineering","text":"Data Engineering Project Activity & Issue Tracking Docs News & Updates","title":"Data Engineering"},{"location":"index.html#purpose-of-this-project","text":"This repository contains my notes for Udacity's Data Engineering Nanodegree The notes are represented in a question-and-answer format to allow for active recall testing If you have completed the degree yourself, you might be interested in testing your knowledge by trying to answer the questions. Regardless of your ability to answer the questions, and the quality of my written answers , the mere exposure and deliberate retrieval practice will help you consolidate your knowledge Have fun!","title":"Purpose of this Project"},{"location":"knowledge/index.html","text":"Quiz yourself :) \u00b6 Info The content is programatically generated. Feel free to submit an issue if you spot formatting errors or other mistakes Table of Contents \u00b6 Data Engineering Nanodegree - Part 1 - Data Modeling Data Engineering Data Modeling Module 1 Introduction To Data Modeling Module 2 Relational Data Models Module 3 Nosql Data Models Acronyms Data Engineering Nanodegree - Part 2 - Cloud Data Warehouses Data Engineering Cloud Data Warehouses Module 1 Introduction To Data Warehouses Module 2 Introduction To Cloud Computing And Aws Module 3 Implementing Data Warehouses On Aws Acronyms Data Engineering Nanodegree - Part 3 - Data Lakes with Spark Data Engineering Data Lakes With Spark Module 4 Data Wrangling With Spark The Power Of Spark Module 5 Introduction To Data Lakes Acronyms Data Engineering Nanodegree - Part 4 - Data Pipelines With Airflow Data Engineering Data Pipelines With Airflow Module 1 Data Pipelines Module 2 Data Quality Module 3 Production Data Pipelines Acronyms Quiz yourself :) Table of Contents","title":"Index"},{"location":"knowledge/index.html#quiz-yourself","text":"Info The content is programatically generated. Feel free to submit an issue if you spot formatting errors or other mistakes","title":"Quiz yourself :)"},{"location":"knowledge/index.html#table-of-contents","text":"Data Engineering Nanodegree - Part 1 - Data Modeling Data Engineering Data Modeling Module 1 Introduction To Data Modeling Module 2 Relational Data Models Module 3 Nosql Data Models Acronyms Data Engineering Nanodegree - Part 2 - Cloud Data Warehouses Data Engineering Cloud Data Warehouses Module 1 Introduction To Data Warehouses Module 2 Introduction To Cloud Computing And Aws Module 3 Implementing Data Warehouses On Aws Acronyms Data Engineering Nanodegree - Part 3 - Data Lakes with Spark Data Engineering Data Lakes With Spark Module 4 Data Wrangling With Spark The Power Of Spark Module 5 Introduction To Data Lakes Acronyms Data Engineering Nanodegree - Part 4 - Data Pipelines With Airflow Data Engineering Data Pipelines With Airflow Module 1 Data Pipelines Module 2 Data Quality Module 3 Production Data Pipelines Acronyms Quiz yourself :) Table of Contents","title":"Table of Contents"},{"location":"knowledge/data_engineering_nanodegree_-_part_1_-_data_modeling.html","text":"Data Engineering Nanodegree - Part 1 - Data Modeling \u00b6 Data Engineering \u00b6 Data Modeling \u00b6 Module 1 \u00b6 Introduction To Data Modeling \u00b6 Q: What is a database ? Answer : A database is a structured repository or organised collection of data It is stored and retrieved electronically for use in applications Remarks : Data can be stored, updated, or deleted from a database. Q: What is a data base management system ( DBMS )? Answer : It is the software that interacts with end users, applications, and the database itself to capture and analyze the data. Remarks : Introduction to DBMS DBMS as per Wikipedia Q: Why can't everything be stored in a giant Excel spreadsheet? Answer : There are limitations to the amount of data that can be stored in an Excel sheet. So, a database helps organize the elements into tables - rows and columns, etc. Also reading and writing operations on a large scale is not possible with an Excel sheet, so it's better to use a database to handle most business functions. Q: Does data modeling happen before you create a database, or is it an iterative process? Answer : It's definitely an iterative process. Data engineers continually reorganize, restructure, and optimize data models to fit the needs of the organization. Q: What is a data model? Answer : It is an abstraction that organizes elements of data and how they will relate to each other. Remarks : Check with wikipedia Q: What is a schema? Answer : The term \"schema\" refers to the organization of data as a blueprint of how the database is constructed (divided into database tables in the case of relational databases). Q: What are advantages of using a relational database ? Answer : Writing SQL queries: With SQL being the most common database query language. Modeling the data not modeling queries Ability to do JOINS Simplicity: If you have a smaller data volume (and not big data) you can use a relational database for its simplicity. ACID Transactions : Allows you to meet a set of properties of database transactions intended to guarantee validity even in the event of errors, power failures, and thus maintain data integrity. Q: Describe atomicity (ACID) Answer : \"The whole transaction is processed or nothing is processed.\" Example : A commonly cited example of an atomic transaction is money transactions between two bank accounts. The transaction of transferring money from one account to the other is made up of two operations. First, you have to withdraw money in one account, and second you have to save the withdrawn money to the second account. An atomic transaction, i.e., when either all operations occur or nothing occurs, keeps the database in a consistent state. This ensures that if either of those two operations (withdrawing money from the 1st account or saving the money to the 2nd account) fail, the money is neither lost nor created. Source Wikipedia for a detailed description of this example. Q: Describe consistency (ACID). Answer : Only transactions that abide by constraints and rules are written into the database, otherwise the database keeps the previous state. Remarks : The data should be correct across all rows and tables. Check out additional information about consistency on Wikipedia . Q: Describe isolation (ACID). Answer : Transactions are processed independentlyand securely, order does not matter. Remarks : A low level of isolation enables many users to access the data simultaneously, however this also increases the possibilities of concurrency effects (e.g., dirty reads or lost updates). On the other hand, a high level of isolation reduces these chances of concurrency effects, but also uses more system resources and transactions blocking each other. Source: Wikipedia Q: Describe durability (ACID). Answer : Completed transactions are saved to database even in cases of system failure. Example : A commonly cited example includes tracking flight seat bookings. So once the flight booking records a confirmed seat booking, the seat remains booked even if a system failure occurs. Source: Wikipedia . Q: When shouldn't you use relational databases? Answer : Have large amounts of data: Relational Databases are not distributed databases and because of this they can only scale vertically by adding more storage in the machine itself. You are limited by how much you can scale and how much data you can store on one machine. You cannot add more machines like you can in NoSQL databases. Need to be able to store different data type formats: Relational databases are not designed to handle unstructured data. Need high throughput -- fast reads: While ACID transactions bring benefits, they also slow down the process of reading and writing data. If you need very fast reads and writes, using a relational database may not suit your needs. Need a flexible schema: Flexible schema can allow for columns to be added that do not have to be used by every row, saving disk space. Need high availability: The fact that relational databases are not distributed (and even when they are, they have a coordinator/worker architecture), they have a single point of failure. When that database goes down, a fail-over to a backup system occurs and takes time. Need horizontal scalability: Horizontal scalability is the ability to add more machines or nodes to a system to increase performance and space for data. Q: What is PostgreSQL Answer : Open source object-relational database system Uses and builds on the SQL language Q: List five common types of NoSQL Databases. Answer : Apache Cassandra (partition row store) MongoDB (document store) DynamoDB (key-value store) Apache HBase (wide column store) Neo4J (graph database) Q: Which concept in Apache Cassandra is similar to a schema in PostgreSQL? Answer : The keyspace ; it is a collection of tables. Q: What are good use cases for NoSQL (and more specifically Apache Cassandra)? Answer : Transaction logging (retail, health care) Internet of Things (IoT) Time series data Any workload that is heavy on writes to the database (since Apache Cassandra is optimized for writes). Example : Uber uses Apache Cassandra for their entire backend. Netflix uses Apache Cassandra to serve all their videos to customers. Q: When to use a NoSQL Database? Answer : Store different data type formats : NoSQL was also created to handle different data configurations: structured, semi-structured, and unstructured data. JSON, XML documents can all be handled easily with NoSQL. Large amounts of data : Relational databases are not distributed databases and because of this they can only scale vertically by adding more storage in the machine itself. NoSQL databases were created to be able to be horizontally scalable. The more servers/systems you add to the database the more data that can be hosted with high availability and low latency (fast reads and writes). Need horizontal scalability : Horizontal scalability is the ability to add more machines or nodes to a system to increase performance and space for data Need high throughput : While ACID transactions bring benefits they also slow down the process of reading and writing data. If you need very fast reads and writes using a relational database may not suit your needs. Need a flexible schema : Flexible schema can allow for columns to be added that do not have to be used by every row, saving disk space. Need high availability : Relational databases have a single point of failure. When that database goes down, a failover to a backup system must happen and takes time. Users are distributed Q: When NOT to use a NoSQL Database? Answer : When you have a small dataset : NoSQL databases were made for big datasets not small datasets and while it works it wasn\u2019t created for that. When you need ACID Transactions : If you need a consistent database with ACID transactions, then most NoSQL databases will not be able to serve this need. NoSQL databases are eventually consistent and do not provide ACID transactions. However, there are exceptions to it. Some non-relational databases like MongoDB can support ACID transactions. When you need the ability to do JOINS across tables : NoSQL does not allow the ability to do JOINS. This is not allowed as this will result in full table scans. If you want to be able to do aggregations and analytics If you have changing business requirements : Ad-hoc queries are possible but difficult as the data model was done to fix particular queries If your queries are not available and you need the flexibility : You need your queries in advance. If those are not available or you will need to be able to have flexibility on how you query your data you might need to stick with a relational database Remarks : Caveats to NoSQL and ACID Transactions There are some NoSQL databases that offer some form of ACID transaction. As of v4.0, MongoDB added multi-document ACID transactions within a single replica set. With their later version, v4.2, they have added multi-document ACID transactions in a sharded/partitioned deployment. Check out the documentation from MongoDB on multi-document ACID transactions Module 2 \u00b6 Relational Data Models \u00b6 Q: What are advantages of relational databases? Answer : Standardization of data model: Once your data is transformed into the rows and columns format, your data is standardized and you can query it with SQL Flexibility in adding and altering tables: Relational databases gives you flexibility to add tables, alter tables, add and remove data. Data Integrity: Data Integrity is the backbone of using a relational database. Structured Query Language (SQL): A standard language can be used to access the data with a predefined language. Simplicity : Data is systematically stored and modeled in tabular format. Intuitive Organization: The spreadsheet format is intuitive to data modeling in relational databases. Q: Describe the difference between OLAP and OLTP. Answer : Online Analytical Processing (OLAP): Databases optimized for these workloads allow for complex analytical and ad hoc queries, including aggregations. These type of databases are optimized for reads. Online Transactional Processing (OLTP): Databases optimized for these workloads allow for less complex queries in large volume. The types of queries for these databases are read, insert, update, and delete. Example : The key to remember the difference between OLAP and OLTP is analytics (A) vs transactions (T). If you want to get the price of a shoe then you are using OLTP (this has very little or no aggregations). If you want to know the total stock of shoes a particular store sold, then this requires using OLAP (since this will require aggregations). Remarks : This Stackoverflow post describes it well. Q: What are the two most important concepts for structuring your database? Answer : Normalization Denormalization Q: What is normalization ? Answer : The process of structuring a relational database in accordance with a series of normal forms in order to reduce data redundancy and increase data integrity . Q: What are the objectives of normal form ? Answer : To free the database from unwanted insertions, updates, & deletion dependencies To reduce the need for refactoring the database as new types of data are introduced To make the relational model more informative to users To make the database neutral to the query statistics Remarks : See this Wikipedia page to learn more. Q: What are the three steps of normalization? Answer : First Normal Form (1NF) Second Normal Form (2NF) Third Normal Form (3NF) Q: How to reach First Normal Form (1NF)? Answer : Atomic values: each cell contains unique and single values Be able to add data without altering tables Separate different relations into different tables Keep relationships between tables together with foreign keys Q: How to reach second normal form ( 2NF )? Answer : First, reach 1NF All columns in the table must rely on the Primary Key Q: How to reach third normal form ( 3NF )? Answer : Must be in 2nd Normal Form No transitive dependencies Remember, transitive dependencies you are trying to maintain is that to get from A-> C, you want to avoid going through B. Q: What is denormalization ? Answer : It is the process of trying to improve the read performance (select) of a database at the expense of losing some write performance (insert, update, delete) by adding redundant copies of data. Remarks : JOINS on the database allow for outstanding flexibility but are extremely slow. If you are dealing with heavy reads on your database, you may want to think about denormalizing your tables. You get your data into normalized form, and then you proceed with denormalization. So, denormalization comes after normalization. Citation for udacity slides: https://en.wikipedia.org/wiki/Denormalization Q: How does the following table look like in 3NF? Answer : No row contains a list of items. For example, the list of songs has been replaced with each song having its own row in the Song table. Transitive dependencies have been removed . For example, albumID is the PRIMARY KEY for the album year in Album Table. Similarly, each of the other tables have a unique primary key that can identify the other values in the table (e.g., song id and song name within Songtable). Song_Table Album_Table Artist_Table Q: What are the two most popular data mart schemas for data warehouses (because of their simplicity) ? Answer : Star Schema Snowflake Schema Q: What is a dimension table? Answer : A dimension is a structure that categorizes facts and measures in order to enable users to answer business questions. Example : Commonly used dimensions are people, products, place and time. Remarks : Note: People and time sometimes are not modeled as dimensions. Q: What is a fact table ? Answer : In data warehousing, a fact table consists of the measurements, metrics or facts of a business process. Example : Example of a star schema; the central table is the fact table By Jesuja - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=3886973 Remarks : It is located at the center of a star schema or a snowflake schema surrounded by dimension tables. Q: What is a star schema ? Answer : It is the simplest style of data mart schema The star schema consists of one or more fact tables referencing any number of dimension tables. Example : By SqlPac at English Wikipedia, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=76901169 Remarks : It is the approach most widely used to develop data warehouses and dimensional data marts. The star schema is an important special case of the snowflake schema, and is more effective for handling simpler queries. Q: What are benefits of star schemas? Answer : Star schemas are denormalized meaning the typical rules of normalization applied to transactional relational databases are relaxed during star-schema design and implementation. The benefits of star-schema denormalization are: Simpler queries \u2013 star-schema join-logic is generally simpler than the join logic required to retrieve data from a highly normalized transactional schema. Simplified business reporting logic \u2013 when compared to highly normalized schemas, the star schema simplifies common business reporting logic, such as period-over-period and as-of reporting. Query performance gains \u2013 star schemas can provide performance enhancements for read-only reporting applications when compared to highly normalized schemas. Fast aggregations \u2013 the simpler queries against a star schema can result in improved performance for aggregation operations. Q: What are drawbacks of star schemas ? Answer : The main disadvantage of the star schema is that it's not as flexible in terms of analytical needs as a normalized data model. Normalized models allow any kind of analytical query to be executed, so long as it follows the business logic defined in the model. Star schemas tend to be more purpose-built toward a particular view of the data, thus not really allowing more complex analytics. Star schemas don't easily support many-to-many relationships between business entities. Typically these relationships are simplified in a star schema in order to conform to the simple dimensional model. Data integrity is not well-enforced due to its denormalized state. One-off inserts and updates can result in data anomalies, which normalized schemas are designed to avoid. Q: What is the snowflake schema ? Answer : It is a logical arrangement of tables in a multidimensional database such that the entity relationship diagram resembles a snowflake shape. The snowflake schema is represented by centralized fact tables which are connected to multiple dimensions. Example : The snowflake schema is a variation of the star schema, featuring normalization of dimension tables. By SqlPac - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=4285113 Remarks : The snowflake schema is similar to the star schema. However, in the snowflake schema, dimensions are normalized into multiple related tables, whereas the star schema's dimensions are denormalized with each dimension represented by a single table. Q: Describe the NOT NULL constraint (SQL) Answer : The NOT NULL constraint indicates that the column cannot contain a null value. Example : Here is the syntax for adding a NOT NULL constraint to the CREATE statement: CREATE TABLE IF NOT EXISTS customer_transactions ( customer_id int NOT NULL, store_id int, spent numeric ); You can add NOT NULL constraints to more than one column. Usually this occurs when you have a COMPOSITE KEY , which will be discussed further below. Here is the syntax for it: CREATE TABLE IF NOT EXISTS customer_transactions ( customer_id int NOT NULL, store_id int NOT NULL, spent numeric ); Q: Describe the UNIQUE constraint (SQL) Answer : The UNIQUE constraint is used to specify that the data across all the rows in one column are unique within the table. The UNIQUE constraint can also be used for multiple columns, so that the combination of the values across those columns will be unique within the table. In this latter case, the values within 1 column do not need to be unique. Example : Let's look at an example. CREATE TABLE IF NOT EXISTS customer_transactions ( customer_id int NOT NULL UNIQUE, store_id int NOT NULL UNIQUE, spent numeric ); Another way to write a UNIQUE constraint is to add a table constraint using commas to separate the columns. CREATE TABLE IF NOT EXISTS customer_transactions ( customer_id int NOT NULL, store_id int NOT NULL, spent numeric, UNIQUE (customer_id, store_id, spent) ); Q: Describe the PRIMARY KEY constraint (SQL) Answer : The PRIMARY KEY constraint is defined on a single column, and every table should contain a primary key. The values in this column uniquely identify the rows in the table. Example : Let's look at the following example: CREATE TABLE IF NOT EXISTS store ( store_id int PRIMARY KEY, store_location_city text, store_location_state text ); Here is an example for a group of columns serving as composite key . CREATE TABLE IF NOT EXISTS customer_transactions ( customer_id int, store_id int, spent numeric, PRIMARY KEY (customer_id, store_id) ); Remarks : If a group of columns are defined as a primary key, they are called a composite key . That means the combination of values in these columns will uniquely identify the rows in the table. By default, the PRIMARY KEY constraint has the unique and not null constraint built into it. Q: What is an upsert in RDBMS language? Answer : In RDBMS language, the term upsert refers to the idea of inserting a new row in an existing table, or updating the row if it already exists in the table. The action of updating or inserting has been described as \"upsert\". Example : Now let's assume that the customer moved and we need to update the customer's address. However we do not want to add a new customer id. In other words, if there is any conflict on the customer_id , we do not want that to change. This would be a good candidate for using the ON CONFLICT DO NOTHING clause. INSERT INTO customer_address (customer_id, customer_street, customer_city, customer_state) VALUES ( 432, '923 Knox Street', 'Albany', 'NY' ) ON CONFLICT (customer_id) DO NOTHING; Now, let's imagine we want to add more details in the existing address for an existing customer. This would be a good candidate for using the ON CONFLICT DO UPDATE clause. INSERT INTO customer_address (customer_id, customer_street) VALUES ( 432, '923 Knox Street, Suite 1' ) ON CONFLICT (customer_id) DO UPDATE SET customer_street = EXCLUDED.customer_street; Remarks : The way this is handled in PostgreSQL is by using the INSERT statement in combination with the ON CONFLICT clause. Q: How can you perform an upsert in PostgreSQL? Answer : By using the INSERT statement in combination with the ON CONFLICT clause. Q: What is the extract , transform , load ( ETL ) process? Answer : It is a general procedure of copying data from one or more sources into a destination system which represents the data differently from the source(s) or in a different context than the source(s). Remarks : The ETL process became a popular concept in the 1970s and is often used in data warehousing. Module 3 \u00b6 Nosql Data Models \u00b6 Q: What is eventual consistency ? Answer : Over time (if no new changes are made) each copy of the data will be the same, but if there are new changes, the data may be different in different locations. The data may be inconsistent for only milliseconds. There are workarounds in place to prevent getting stale data. Remarks : See also: https://en.wikipedia.org/wiki/Eventual_consistency Q: What does the CAP theorem state? Answer : It states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees: Consistency Availability Partition tolerance Q: What do consistency , availability and partition tolerance mean in the CAP theorem ? Answer : Consistency : Every read receives the most recent write or an error. Availability : Every request receives a (non-error) response, without the guarantee that it contains the most recent write. Partition tolerance : The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes. Q: How is CAP Consistency different from ACID Consistency? Answer : ACID consistency is all about database rules. If a schema declares that a value must be unique, then a consistent system will enforce uniqueness of that value across all operations. If a foreign key implies deleting one row will delete related rows, then a consistent system will ensure the state can\u2019t contain related rows once the base row is deleted. CAP consistency promises that every replica of the same logical value, spread across nodes in a distributed system, has the same exact value at all times. Note that this is a logical guarantee, rather than a physical one. Due to the speed of light, it may take some non-zero time to replicate values across a cluster. The cluster can still present a logical view by preventing clients from viewing different values at different nodes. Remarks : Source: https://www.voltdb.com/blog/2015/10/disambiguating-acid-cap/ Q: List a few important rules for data modeling in Apache Cassandra. Answer : Denormalization must be done for fast reads ALWAYS think Queries first One table per query is a great strategy Apache Cassandra does not allow for JOINs between tables Q: Why do we need to use a WHERE statement in Apache Cassandra? Answer : The WHERE statement is allowing us to do the fast reads. With Apache Cassandra, we are talking about big data -- think terabytes of data -- so we are making it fast for read purposes. Data is spread across all the nodes. By using the WHERE statement, we know which node to go to, from which node to get that data and serve it back. Example : For example, imagine we have 10 years of data on 10 nodes or servers. So 1 year's data is on a separate node. By using the WHERE year = 1 statement we know which node to visit fast to pull the data from. Q: Can you do SELECT * FROM myTable in Apache Cassandra? Answer : It is highly discouraged because performance will be slow (or it may just fail) but it is possible with the \"ALLOW FILTERING\" configuration. Q: What is the purpose of the PRIMARY KEY in Apache Cassandra? Answer : It it used to uniquely identify each row and to specify how the data is distributed across the nodes (or servers) in our system. Q: What is the difference between a simple PRIMARY KEY and a composite PRIMARY KEY ? Answer : A Simple PRIMARY KEY is just one column that is also the PARTITION KEY. A Composite PRIMARY KEY is made up of more than one column and will assist in creating a unique value in your retrieval queries. Q: What is the PRIMARY KEY made up in Cassandra? Answer : It is made up of either just the PARTITION KEY or it may additionaly include one or more CLUSTERING COLUMNS . Acronyms \u00b6 DBMS : Database Management System DDL : Data Definition Language RDBMS : Relational Database Management System SQL : Structured Query Language ACID : Atomicity, Consistency, Isolation, Durability CQL : Cassandra Query Language ERD : Entity Relationship Diagram OLAP : Online Analytical Processing OLTP : Online Transaction Processing ETL : Extract, Transform, Load","title":"Data Engineering Nanodegree - Part 1 - Data Modeling"},{"location":"knowledge/data_engineering_nanodegree_-_part_1_-_data_modeling.html#data-engineering-nanodegree-part-1-data-modeling","text":"","title":"Data Engineering Nanodegree - Part 1 - Data Modeling"},{"location":"knowledge/data_engineering_nanodegree_-_part_1_-_data_modeling.html#data-engineering","text":"","title":"Data Engineering"},{"location":"knowledge/data_engineering_nanodegree_-_part_1_-_data_modeling.html#data-modeling","text":"","title":"Data Modeling"},{"location":"knowledge/data_engineering_nanodegree_-_part_1_-_data_modeling.html#module-1","text":"","title":"Module 1"},{"location":"knowledge/data_engineering_nanodegree_-_part_1_-_data_modeling.html#introduction-to-data-modeling","text":"Q: What is a database ? Answer : A database is a structured repository or organised collection of data It is stored and retrieved electronically for use in applications Remarks : Data can be stored, updated, or deleted from a database. Q: What is a data base management system ( DBMS )? Answer : It is the software that interacts with end users, applications, and the database itself to capture and analyze the data. Remarks : Introduction to DBMS DBMS as per Wikipedia Q: Why can't everything be stored in a giant Excel spreadsheet? Answer : There are limitations to the amount of data that can be stored in an Excel sheet. So, a database helps organize the elements into tables - rows and columns, etc. Also reading and writing operations on a large scale is not possible with an Excel sheet, so it's better to use a database to handle most business functions. Q: Does data modeling happen before you create a database, or is it an iterative process? Answer : It's definitely an iterative process. Data engineers continually reorganize, restructure, and optimize data models to fit the needs of the organization. Q: What is a data model? Answer : It is an abstraction that organizes elements of data and how they will relate to each other. Remarks : Check with wikipedia Q: What is a schema? Answer : The term \"schema\" refers to the organization of data as a blueprint of how the database is constructed (divided into database tables in the case of relational databases). Q: What are advantages of using a relational database ? Answer : Writing SQL queries: With SQL being the most common database query language. Modeling the data not modeling queries Ability to do JOINS Simplicity: If you have a smaller data volume (and not big data) you can use a relational database for its simplicity. ACID Transactions : Allows you to meet a set of properties of database transactions intended to guarantee validity even in the event of errors, power failures, and thus maintain data integrity. Q: Describe atomicity (ACID) Answer : \"The whole transaction is processed or nothing is processed.\" Example : A commonly cited example of an atomic transaction is money transactions between two bank accounts. The transaction of transferring money from one account to the other is made up of two operations. First, you have to withdraw money in one account, and second you have to save the withdrawn money to the second account. An atomic transaction, i.e., when either all operations occur or nothing occurs, keeps the database in a consistent state. This ensures that if either of those two operations (withdrawing money from the 1st account or saving the money to the 2nd account) fail, the money is neither lost nor created. Source Wikipedia for a detailed description of this example. Q: Describe consistency (ACID). Answer : Only transactions that abide by constraints and rules are written into the database, otherwise the database keeps the previous state. Remarks : The data should be correct across all rows and tables. Check out additional information about consistency on Wikipedia . Q: Describe isolation (ACID). Answer : Transactions are processed independentlyand securely, order does not matter. Remarks : A low level of isolation enables many users to access the data simultaneously, however this also increases the possibilities of concurrency effects (e.g., dirty reads or lost updates). On the other hand, a high level of isolation reduces these chances of concurrency effects, but also uses more system resources and transactions blocking each other. Source: Wikipedia Q: Describe durability (ACID). Answer : Completed transactions are saved to database even in cases of system failure. Example : A commonly cited example includes tracking flight seat bookings. So once the flight booking records a confirmed seat booking, the seat remains booked even if a system failure occurs. Source: Wikipedia . Q: When shouldn't you use relational databases? Answer : Have large amounts of data: Relational Databases are not distributed databases and because of this they can only scale vertically by adding more storage in the machine itself. You are limited by how much you can scale and how much data you can store on one machine. You cannot add more machines like you can in NoSQL databases. Need to be able to store different data type formats: Relational databases are not designed to handle unstructured data. Need high throughput -- fast reads: While ACID transactions bring benefits, they also slow down the process of reading and writing data. If you need very fast reads and writes, using a relational database may not suit your needs. Need a flexible schema: Flexible schema can allow for columns to be added that do not have to be used by every row, saving disk space. Need high availability: The fact that relational databases are not distributed (and even when they are, they have a coordinator/worker architecture), they have a single point of failure. When that database goes down, a fail-over to a backup system occurs and takes time. Need horizontal scalability: Horizontal scalability is the ability to add more machines or nodes to a system to increase performance and space for data. Q: What is PostgreSQL Answer : Open source object-relational database system Uses and builds on the SQL language Q: List five common types of NoSQL Databases. Answer : Apache Cassandra (partition row store) MongoDB (document store) DynamoDB (key-value store) Apache HBase (wide column store) Neo4J (graph database) Q: Which concept in Apache Cassandra is similar to a schema in PostgreSQL? Answer : The keyspace ; it is a collection of tables. Q: What are good use cases for NoSQL (and more specifically Apache Cassandra)? Answer : Transaction logging (retail, health care) Internet of Things (IoT) Time series data Any workload that is heavy on writes to the database (since Apache Cassandra is optimized for writes). Example : Uber uses Apache Cassandra for their entire backend. Netflix uses Apache Cassandra to serve all their videos to customers. Q: When to use a NoSQL Database? Answer : Store different data type formats : NoSQL was also created to handle different data configurations: structured, semi-structured, and unstructured data. JSON, XML documents can all be handled easily with NoSQL. Large amounts of data : Relational databases are not distributed databases and because of this they can only scale vertically by adding more storage in the machine itself. NoSQL databases were created to be able to be horizontally scalable. The more servers/systems you add to the database the more data that can be hosted with high availability and low latency (fast reads and writes). Need horizontal scalability : Horizontal scalability is the ability to add more machines or nodes to a system to increase performance and space for data Need high throughput : While ACID transactions bring benefits they also slow down the process of reading and writing data. If you need very fast reads and writes using a relational database may not suit your needs. Need a flexible schema : Flexible schema can allow for columns to be added that do not have to be used by every row, saving disk space. Need high availability : Relational databases have a single point of failure. When that database goes down, a failover to a backup system must happen and takes time. Users are distributed Q: When NOT to use a NoSQL Database? Answer : When you have a small dataset : NoSQL databases were made for big datasets not small datasets and while it works it wasn\u2019t created for that. When you need ACID Transactions : If you need a consistent database with ACID transactions, then most NoSQL databases will not be able to serve this need. NoSQL databases are eventually consistent and do not provide ACID transactions. However, there are exceptions to it. Some non-relational databases like MongoDB can support ACID transactions. When you need the ability to do JOINS across tables : NoSQL does not allow the ability to do JOINS. This is not allowed as this will result in full table scans. If you want to be able to do aggregations and analytics If you have changing business requirements : Ad-hoc queries are possible but difficult as the data model was done to fix particular queries If your queries are not available and you need the flexibility : You need your queries in advance. If those are not available or you will need to be able to have flexibility on how you query your data you might need to stick with a relational database Remarks : Caveats to NoSQL and ACID Transactions There are some NoSQL databases that offer some form of ACID transaction. As of v4.0, MongoDB added multi-document ACID transactions within a single replica set. With their later version, v4.2, they have added multi-document ACID transactions in a sharded/partitioned deployment. Check out the documentation from MongoDB on multi-document ACID transactions","title":"Introduction To Data Modeling"},{"location":"knowledge/data_engineering_nanodegree_-_part_1_-_data_modeling.html#module-2","text":"","title":"Module 2"},{"location":"knowledge/data_engineering_nanodegree_-_part_1_-_data_modeling.html#relational-data-models","text":"Q: What are advantages of relational databases? Answer : Standardization of data model: Once your data is transformed into the rows and columns format, your data is standardized and you can query it with SQL Flexibility in adding and altering tables: Relational databases gives you flexibility to add tables, alter tables, add and remove data. Data Integrity: Data Integrity is the backbone of using a relational database. Structured Query Language (SQL): A standard language can be used to access the data with a predefined language. Simplicity : Data is systematically stored and modeled in tabular format. Intuitive Organization: The spreadsheet format is intuitive to data modeling in relational databases. Q: Describe the difference between OLAP and OLTP. Answer : Online Analytical Processing (OLAP): Databases optimized for these workloads allow for complex analytical and ad hoc queries, including aggregations. These type of databases are optimized for reads. Online Transactional Processing (OLTP): Databases optimized for these workloads allow for less complex queries in large volume. The types of queries for these databases are read, insert, update, and delete. Example : The key to remember the difference between OLAP and OLTP is analytics (A) vs transactions (T). If you want to get the price of a shoe then you are using OLTP (this has very little or no aggregations). If you want to know the total stock of shoes a particular store sold, then this requires using OLAP (since this will require aggregations). Remarks : This Stackoverflow post describes it well. Q: What are the two most important concepts for structuring your database? Answer : Normalization Denormalization Q: What is normalization ? Answer : The process of structuring a relational database in accordance with a series of normal forms in order to reduce data redundancy and increase data integrity . Q: What are the objectives of normal form ? Answer : To free the database from unwanted insertions, updates, & deletion dependencies To reduce the need for refactoring the database as new types of data are introduced To make the relational model more informative to users To make the database neutral to the query statistics Remarks : See this Wikipedia page to learn more. Q: What are the three steps of normalization? Answer : First Normal Form (1NF) Second Normal Form (2NF) Third Normal Form (3NF) Q: How to reach First Normal Form (1NF)? Answer : Atomic values: each cell contains unique and single values Be able to add data without altering tables Separate different relations into different tables Keep relationships between tables together with foreign keys Q: How to reach second normal form ( 2NF )? Answer : First, reach 1NF All columns in the table must rely on the Primary Key Q: How to reach third normal form ( 3NF )? Answer : Must be in 2nd Normal Form No transitive dependencies Remember, transitive dependencies you are trying to maintain is that to get from A-> C, you want to avoid going through B. Q: What is denormalization ? Answer : It is the process of trying to improve the read performance (select) of a database at the expense of losing some write performance (insert, update, delete) by adding redundant copies of data. Remarks : JOINS on the database allow for outstanding flexibility but are extremely slow. If you are dealing with heavy reads on your database, you may want to think about denormalizing your tables. You get your data into normalized form, and then you proceed with denormalization. So, denormalization comes after normalization. Citation for udacity slides: https://en.wikipedia.org/wiki/Denormalization Q: How does the following table look like in 3NF? Answer : No row contains a list of items. For example, the list of songs has been replaced with each song having its own row in the Song table. Transitive dependencies have been removed . For example, albumID is the PRIMARY KEY for the album year in Album Table. Similarly, each of the other tables have a unique primary key that can identify the other values in the table (e.g., song id and song name within Songtable). Song_Table Album_Table Artist_Table Q: What are the two most popular data mart schemas for data warehouses (because of their simplicity) ? Answer : Star Schema Snowflake Schema Q: What is a dimension table? Answer : A dimension is a structure that categorizes facts and measures in order to enable users to answer business questions. Example : Commonly used dimensions are people, products, place and time. Remarks : Note: People and time sometimes are not modeled as dimensions. Q: What is a fact table ? Answer : In data warehousing, a fact table consists of the measurements, metrics or facts of a business process. Example : Example of a star schema; the central table is the fact table By Jesuja - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=3886973 Remarks : It is located at the center of a star schema or a snowflake schema surrounded by dimension tables. Q: What is a star schema ? Answer : It is the simplest style of data mart schema The star schema consists of one or more fact tables referencing any number of dimension tables. Example : By SqlPac at English Wikipedia, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=76901169 Remarks : It is the approach most widely used to develop data warehouses and dimensional data marts. The star schema is an important special case of the snowflake schema, and is more effective for handling simpler queries. Q: What are benefits of star schemas? Answer : Star schemas are denormalized meaning the typical rules of normalization applied to transactional relational databases are relaxed during star-schema design and implementation. The benefits of star-schema denormalization are: Simpler queries \u2013 star-schema join-logic is generally simpler than the join logic required to retrieve data from a highly normalized transactional schema. Simplified business reporting logic \u2013 when compared to highly normalized schemas, the star schema simplifies common business reporting logic, such as period-over-period and as-of reporting. Query performance gains \u2013 star schemas can provide performance enhancements for read-only reporting applications when compared to highly normalized schemas. Fast aggregations \u2013 the simpler queries against a star schema can result in improved performance for aggregation operations. Q: What are drawbacks of star schemas ? Answer : The main disadvantage of the star schema is that it's not as flexible in terms of analytical needs as a normalized data model. Normalized models allow any kind of analytical query to be executed, so long as it follows the business logic defined in the model. Star schemas tend to be more purpose-built toward a particular view of the data, thus not really allowing more complex analytics. Star schemas don't easily support many-to-many relationships between business entities. Typically these relationships are simplified in a star schema in order to conform to the simple dimensional model. Data integrity is not well-enforced due to its denormalized state. One-off inserts and updates can result in data anomalies, which normalized schemas are designed to avoid. Q: What is the snowflake schema ? Answer : It is a logical arrangement of tables in a multidimensional database such that the entity relationship diagram resembles a snowflake shape. The snowflake schema is represented by centralized fact tables which are connected to multiple dimensions. Example : The snowflake schema is a variation of the star schema, featuring normalization of dimension tables. By SqlPac - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=4285113 Remarks : The snowflake schema is similar to the star schema. However, in the snowflake schema, dimensions are normalized into multiple related tables, whereas the star schema's dimensions are denormalized with each dimension represented by a single table. Q: Describe the NOT NULL constraint (SQL) Answer : The NOT NULL constraint indicates that the column cannot contain a null value. Example : Here is the syntax for adding a NOT NULL constraint to the CREATE statement: CREATE TABLE IF NOT EXISTS customer_transactions ( customer_id int NOT NULL, store_id int, spent numeric ); You can add NOT NULL constraints to more than one column. Usually this occurs when you have a COMPOSITE KEY , which will be discussed further below. Here is the syntax for it: CREATE TABLE IF NOT EXISTS customer_transactions ( customer_id int NOT NULL, store_id int NOT NULL, spent numeric ); Q: Describe the UNIQUE constraint (SQL) Answer : The UNIQUE constraint is used to specify that the data across all the rows in one column are unique within the table. The UNIQUE constraint can also be used for multiple columns, so that the combination of the values across those columns will be unique within the table. In this latter case, the values within 1 column do not need to be unique. Example : Let's look at an example. CREATE TABLE IF NOT EXISTS customer_transactions ( customer_id int NOT NULL UNIQUE, store_id int NOT NULL UNIQUE, spent numeric ); Another way to write a UNIQUE constraint is to add a table constraint using commas to separate the columns. CREATE TABLE IF NOT EXISTS customer_transactions ( customer_id int NOT NULL, store_id int NOT NULL, spent numeric, UNIQUE (customer_id, store_id, spent) ); Q: Describe the PRIMARY KEY constraint (SQL) Answer : The PRIMARY KEY constraint is defined on a single column, and every table should contain a primary key. The values in this column uniquely identify the rows in the table. Example : Let's look at the following example: CREATE TABLE IF NOT EXISTS store ( store_id int PRIMARY KEY, store_location_city text, store_location_state text ); Here is an example for a group of columns serving as composite key . CREATE TABLE IF NOT EXISTS customer_transactions ( customer_id int, store_id int, spent numeric, PRIMARY KEY (customer_id, store_id) ); Remarks : If a group of columns are defined as a primary key, they are called a composite key . That means the combination of values in these columns will uniquely identify the rows in the table. By default, the PRIMARY KEY constraint has the unique and not null constraint built into it. Q: What is an upsert in RDBMS language? Answer : In RDBMS language, the term upsert refers to the idea of inserting a new row in an existing table, or updating the row if it already exists in the table. The action of updating or inserting has been described as \"upsert\". Example : Now let's assume that the customer moved and we need to update the customer's address. However we do not want to add a new customer id. In other words, if there is any conflict on the customer_id , we do not want that to change. This would be a good candidate for using the ON CONFLICT DO NOTHING clause. INSERT INTO customer_address (customer_id, customer_street, customer_city, customer_state) VALUES ( 432, '923 Knox Street', 'Albany', 'NY' ) ON CONFLICT (customer_id) DO NOTHING; Now, let's imagine we want to add more details in the existing address for an existing customer. This would be a good candidate for using the ON CONFLICT DO UPDATE clause. INSERT INTO customer_address (customer_id, customer_street) VALUES ( 432, '923 Knox Street, Suite 1' ) ON CONFLICT (customer_id) DO UPDATE SET customer_street = EXCLUDED.customer_street; Remarks : The way this is handled in PostgreSQL is by using the INSERT statement in combination with the ON CONFLICT clause. Q: How can you perform an upsert in PostgreSQL? Answer : By using the INSERT statement in combination with the ON CONFLICT clause. Q: What is the extract , transform , load ( ETL ) process? Answer : It is a general procedure of copying data from one or more sources into a destination system which represents the data differently from the source(s) or in a different context than the source(s). Remarks : The ETL process became a popular concept in the 1970s and is often used in data warehousing.","title":"Relational Data Models"},{"location":"knowledge/data_engineering_nanodegree_-_part_1_-_data_modeling.html#module-3","text":"","title":"Module 3"},{"location":"knowledge/data_engineering_nanodegree_-_part_1_-_data_modeling.html#nosql-data-models","text":"Q: What is eventual consistency ? Answer : Over time (if no new changes are made) each copy of the data will be the same, but if there are new changes, the data may be different in different locations. The data may be inconsistent for only milliseconds. There are workarounds in place to prevent getting stale data. Remarks : See also: https://en.wikipedia.org/wiki/Eventual_consistency Q: What does the CAP theorem state? Answer : It states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees: Consistency Availability Partition tolerance Q: What do consistency , availability and partition tolerance mean in the CAP theorem ? Answer : Consistency : Every read receives the most recent write or an error. Availability : Every request receives a (non-error) response, without the guarantee that it contains the most recent write. Partition tolerance : The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes. Q: How is CAP Consistency different from ACID Consistency? Answer : ACID consistency is all about database rules. If a schema declares that a value must be unique, then a consistent system will enforce uniqueness of that value across all operations. If a foreign key implies deleting one row will delete related rows, then a consistent system will ensure the state can\u2019t contain related rows once the base row is deleted. CAP consistency promises that every replica of the same logical value, spread across nodes in a distributed system, has the same exact value at all times. Note that this is a logical guarantee, rather than a physical one. Due to the speed of light, it may take some non-zero time to replicate values across a cluster. The cluster can still present a logical view by preventing clients from viewing different values at different nodes. Remarks : Source: https://www.voltdb.com/blog/2015/10/disambiguating-acid-cap/ Q: List a few important rules for data modeling in Apache Cassandra. Answer : Denormalization must be done for fast reads ALWAYS think Queries first One table per query is a great strategy Apache Cassandra does not allow for JOINs between tables Q: Why do we need to use a WHERE statement in Apache Cassandra? Answer : The WHERE statement is allowing us to do the fast reads. With Apache Cassandra, we are talking about big data -- think terabytes of data -- so we are making it fast for read purposes. Data is spread across all the nodes. By using the WHERE statement, we know which node to go to, from which node to get that data and serve it back. Example : For example, imagine we have 10 years of data on 10 nodes or servers. So 1 year's data is on a separate node. By using the WHERE year = 1 statement we know which node to visit fast to pull the data from. Q: Can you do SELECT * FROM myTable in Apache Cassandra? Answer : It is highly discouraged because performance will be slow (or it may just fail) but it is possible with the \"ALLOW FILTERING\" configuration. Q: What is the purpose of the PRIMARY KEY in Apache Cassandra? Answer : It it used to uniquely identify each row and to specify how the data is distributed across the nodes (or servers) in our system. Q: What is the difference between a simple PRIMARY KEY and a composite PRIMARY KEY ? Answer : A Simple PRIMARY KEY is just one column that is also the PARTITION KEY. A Composite PRIMARY KEY is made up of more than one column and will assist in creating a unique value in your retrieval queries. Q: What is the PRIMARY KEY made up in Cassandra? Answer : It is made up of either just the PARTITION KEY or it may additionaly include one or more CLUSTERING COLUMNS .","title":"Nosql Data Models"},{"location":"knowledge/data_engineering_nanodegree_-_part_1_-_data_modeling.html#acronyms","text":"DBMS : Database Management System DDL : Data Definition Language RDBMS : Relational Database Management System SQL : Structured Query Language ACID : Atomicity, Consistency, Isolation, Durability CQL : Cassandra Query Language ERD : Entity Relationship Diagram OLAP : Online Analytical Processing OLTP : Online Transaction Processing ETL : Extract, Transform, Load","title":"Acronyms"},{"location":"knowledge/data_engineering_nanodegree_-_part_2_-_cloud_data_warehouses.html","text":"Data Engineering Nanodegree - Part 2 - Cloud Data Warehouses \u00b6 Data Engineering \u00b6 Cloud Data Warehouses \u00b6 Module 1 \u00b6 Introduction To Data Warehouses \u00b6 Q: Give some examples for operational and for analytical business processes . Answer : Operational processes ( make it work ) Find goods & make orders (for customers) Stock and find goods (for inventory staff) Pick up & deliver goods (for delivery staff) Analytical processes ( what is going on? ) Assess the performance of sales staff (for HR) See the effect of different sales channels (for marketing) Monitor sales growth (for marketing) Example : Q: What is a data warehouse ? Answer : DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place that are used for creating analytical reports for workers throughout the enterprise. Example : The basic architecture of a data warehouse: By Soha jamil - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=46448452 Remarks : Alternative definitions: \"A copy of transaction data specifically structured for query and analysis\" (Kimball) \"A subject-oriented , integrated , nonvolatile , and time-variant collection of data in support of management's decisions\" (Inmon) \"A system that retrieves and consolidates data periodically from the source systems into a dimensional or normalized data store. It usually keeps years of history and is queried for business intelligence or other analytical activities . It is typicall updated in batches , not every time a transaction happens in the source system.\" (Rainardi) Q: What are drawbacks of using the same database for both OLAP and OLTP ? Answer : The database schema will be hard to understand for business analysts The analytical queries will be slow as we will have to perform lots of table joins Remarks : However, for small databases, it might be ok. Q: Which constraints need to be satisfied for an entry to be a fact? Answer : It must be numeric , and additive . Q: Which schema is easier to use for a business user: 3NF schema or star schema ? Answer : The star schema . Q: List four DWH architectures. Answer : Kimball's bus architecture Independent Data Marts Inmon's Corporate Information Factory (CIF) Hybrid Bus & CIF Q: What is an OLAP cube ? Answer : It is an aggregation of a fact metric on a number of dimensions. Example : Schematisches Beispiel eines Dimensionsw\u00fcrfels mit drei Dimensionen (Data Cube) Von Mantronic - selbst erstellt, CC BY-SA 3.0, https://de.wikipedia.org/w/index.php?curid=2852018 Q: Describe the two approaches of serving OLAP cubes. Answer : MOLAP : Pre-aggregrates the OLAP cubes and saves them on a special purpose non-relational database. ROLAP : Computes the OLAP cubes on the fly from the existing relational databases where the dimensional model resides. Q: What are the main characteristics of the Kimball Bus architecture ? Answer : Data is kept in a common dimension data model shared across different departments (It does not allow for individual department specific data modeling requirements). Data is not kept at the aggregate level, but rather at the atomic level It is organized by business processes, and used by different departments Q: How do independent data marts differ from the Kimball bus structure? Answer : Every department has independent ETL processes & dimensional models These separate & smaller dimensional models are called data marts . There are different fact tables for the same events ( no conformed dimensions ) Q: What are the main characteristics of Inmon's corporate Information factory ( CIF )? Answer : It contains an enterprise data warehouse that provides a normalized data architecture before individual departments build on it The data marts use a source 3NF model and add denormalization based on department needs Q: Describe slicing (OLAP cube operation). Answer : It's reducing \\(N\\) dimensions to \\(N-1\\) dimensions by restricting one dimension to a single value. Example : By Original: InfopedianDerivative work: Jay - OLAP slicing.png, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=105680876 Q: Describe dicing (OLAP cube operation). Answer : It's computing a sub-cube by picking specific values of multiple dimensions. Example : By Original: InfopedianDerivative work: Jay - OLAP dicing.png, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=105725772 Q: Describe rolling-up (OLAP cube operation). Answer : It's aggregating or combining values and reducing the number of rows or columns. Q: Describe drilling-down (OLAP cube operation). Answer : It's decomposing values and increasing the number of rows or columns. Example : The picture shows a drill-down operation: The analyst moves from the summary category \"Outdoor-Schutzausr\u00fcstung\" to see the sales figures for the individual products. Von Infopedian - own illustration, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=14789910 Q: Describe the GROUP BY CUBE clause. Answer : PostgreSQL CUBE is a subclause of the GROUP BY clause. The CUBE allows you to generate multiple grouping sets. Example : SELECT c1, c2, c3, aggregate (c4) FROM table_name GROUP BY CUBE (c1, c2, c3); would generate all possible grouping sets based on the dimension columns specified in CUBE. It's a short way to define multiple grouping sets: CUBE(c1,c2,c3) GROUPING SETS ( (c1,c2,c3), (c1,c2), (c1,c3), (c2,c3), (c1), (c2), (c3), () ) Module 2 \u00b6 Introduction To Cloud Computing And Aws \u00b6 Q: What is cloud computing ? Answer : It is the practice of using a network of remote servers hosted on the internet to store , manage , and process data , rather than a local server or a personal computer. Q: What are advantages of cloud computing ? Answer : Eliminate need to invest in costly hardware upfront It's significantly faster provisioning the resources you need through the cloud versus the time it would take to gather and build up the hardware you'd need to provide the same support You can provide efficient access to your applications around the world by spreading your deployments to multiple regions. Q: In which three ways can AWS services be accessed? Answer : AWS Management Console , which is the web user interface. The AWS CLI is a useful way to control and automate your services with code SDK s allow you to easily integrate services with your applications through APIs built around specific languages and platforms. Module 3 \u00b6 Implementing Data Warehouses On Aws \u00b6 Q: Give the main technological characteristics of Redshift. Answer : It is column-oriented It is cloud-managed It provides massively parallel processing (MPP) Q: What is the number of nodes in a Redshift cluster equal to? Answer : The number of AWS EC2 instances used in the cluster. Q: What is each slice in a Redshift cluster? Answer : At least 1 CPU with dedicated storage and memory for the slice. Q: If we have a Redshift cluster with 4 nodes, each containing 8 slices, i.e. the cluster collectively offers 32 slices. What is the maximum number of partitions per table? Answer : 32 Remarks : The total number of slices in a cluster is our unit of parallelism and it is equal to the sum of all slices on the cluster. Q: In the ETL implementation on AWS, what is the purpose of the EC2 instance (ETL server)? Answer : It just acts as a client to RDS and Redshift to issue COPY commands Because we store all the data in S3 buckets, we do not need storage on the EC2 machine Q: What are advantages of using S3 for ETL storage? (Compared to storing the data in our own EC2 instance) Answer : S3 is AWS-managed, we don't need to worry about storage reliability. By using S3, we only pay for the storage we use, By using S3, we don't need to worry about not having enough storage. Q: Why do we might need to copy data already stored in S3 to another S3 staging bucket during the ETL process? Answer : Because we will most likely transform the data before inserting it into the DWH. Q: Which method is faster for ingesting data in a sql database? Using the INSERT command in a loop or using bulk insertion with the COPY command? Answer : Bulk insertion with the COPY command. Q: Why should we split a table into multiple files before ingesting them into Redshift? Answer : Because this way, we can execute multiple simultaneous COPY commands. Remarks : Each Redshift slice will act as a separate worker and will ingest the split of a file in parallel, so the process will complete much faster. Q: If you have multiple files that belong to the same table names as follows: s3://mySource/day1-sales.csv.gz s3://mySource/day2-sales.csv.gz ... Which method are you going to use to ingest the files from S3 into Redshift? Answer : You would need to create a manifest file. Remarks : Since the files have a common suffix and not a common prefix, we actually need to create a manifest specifying the list of files. If they were named: s3://mySource/sales-day1.csv.gz s3://mySource/sales-day2.csv.gz we could have relied on the existence of a common prefix. Q: What are advantages of infrastructure-as-code over creating infrastructure by clicking-around? Answer : Sharing : one can share all the steps with others easily Reproducibility : one can be sure that no steps are forgotten Multiple deployments : one can create a test environment identical to the production environment. Maintainability : if a change is needed, one can keep track of the changes by comparing code. Remarks : Sharing, Reproducibility, Multiple Deployments & Maintainability are all advantages of IaC. Q: What is Boto3? Answer : Boto3 is a Python SDK for programmatically accessing AWS. It enables developers to create, configure, and manage AWS services. Remarks : You can find the documentation for Boto3 here . Q: Which two strategies exist to optimize table design (slicing)? Answer : Distribution style Sorting Key Q: What is a drawback of joining 2 tables distributed using an EVEN strategy? Answer : It is slow because records need to be shuffled to put together the join result. Example : For example, a given key (say key=2532) of table 1 will not be on the same slice as the corresponding record in table 2, so the record will be copied (shuffled) between slices on different nodes, which results in slow performance. Q: What does the EVEN key distribution style mean? Answer : It means that a table is distributed across slices such that all slices have approximately the equal amount of records from the partitioned table. Q: What does the ALL distribution style do to a table? Answer : It replicates a table on all slices. Remarks : This is especially useful for small tables that are used often. Q: What's another common name for the ALL distribution style? Answer : \" Broadcasting \" is a common term used to the implementation of the ALL distribution style. Q: How does the AUTO distribution style work? Answer : It leaves the distribution decision to Redshift \"Small enough\" tables are distributed with an ALL strategy Large tables are distributed with an EVEN strategy Q: How does the key distribution style distribute the tables? Answer : It places rows with similar values on the same slice. Q: What are the four types of distribution styles in Redshift? Answer : Even All Auto Key Acronyms \u00b6 DWH : Data Warehouse CIF : Corporate Information Factory ROLAP : Relational OLAP MOLAP : Multidimensional OLAP SDK : Software Development Kit","title":"Data Engineering Nanodegree - Part 2 - Cloud Data Warehouses"},{"location":"knowledge/data_engineering_nanodegree_-_part_2_-_cloud_data_warehouses.html#data-engineering-nanodegree-part-2-cloud-data-warehouses","text":"","title":"Data Engineering Nanodegree - Part 2 - Cloud Data Warehouses"},{"location":"knowledge/data_engineering_nanodegree_-_part_2_-_cloud_data_warehouses.html#data-engineering","text":"","title":"Data Engineering"},{"location":"knowledge/data_engineering_nanodegree_-_part_2_-_cloud_data_warehouses.html#cloud-data-warehouses","text":"","title":"Cloud Data Warehouses"},{"location":"knowledge/data_engineering_nanodegree_-_part_2_-_cloud_data_warehouses.html#module-1","text":"","title":"Module 1"},{"location":"knowledge/data_engineering_nanodegree_-_part_2_-_cloud_data_warehouses.html#introduction-to-data-warehouses","text":"Q: Give some examples for operational and for analytical business processes . Answer : Operational processes ( make it work ) Find goods & make orders (for customers) Stock and find goods (for inventory staff) Pick up & deliver goods (for delivery staff) Analytical processes ( what is going on? ) Assess the performance of sales staff (for HR) See the effect of different sales channels (for marketing) Monitor sales growth (for marketing) Example : Q: What is a data warehouse ? Answer : DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place that are used for creating analytical reports for workers throughout the enterprise. Example : The basic architecture of a data warehouse: By Soha jamil - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=46448452 Remarks : Alternative definitions: \"A copy of transaction data specifically structured for query and analysis\" (Kimball) \"A subject-oriented , integrated , nonvolatile , and time-variant collection of data in support of management's decisions\" (Inmon) \"A system that retrieves and consolidates data periodically from the source systems into a dimensional or normalized data store. It usually keeps years of history and is queried for business intelligence or other analytical activities . It is typicall updated in batches , not every time a transaction happens in the source system.\" (Rainardi) Q: What are drawbacks of using the same database for both OLAP and OLTP ? Answer : The database schema will be hard to understand for business analysts The analytical queries will be slow as we will have to perform lots of table joins Remarks : However, for small databases, it might be ok. Q: Which constraints need to be satisfied for an entry to be a fact? Answer : It must be numeric , and additive . Q: Which schema is easier to use for a business user: 3NF schema or star schema ? Answer : The star schema . Q: List four DWH architectures. Answer : Kimball's bus architecture Independent Data Marts Inmon's Corporate Information Factory (CIF) Hybrid Bus & CIF Q: What is an OLAP cube ? Answer : It is an aggregation of a fact metric on a number of dimensions. Example : Schematisches Beispiel eines Dimensionsw\u00fcrfels mit drei Dimensionen (Data Cube) Von Mantronic - selbst erstellt, CC BY-SA 3.0, https://de.wikipedia.org/w/index.php?curid=2852018 Q: Describe the two approaches of serving OLAP cubes. Answer : MOLAP : Pre-aggregrates the OLAP cubes and saves them on a special purpose non-relational database. ROLAP : Computes the OLAP cubes on the fly from the existing relational databases where the dimensional model resides. Q: What are the main characteristics of the Kimball Bus architecture ? Answer : Data is kept in a common dimension data model shared across different departments (It does not allow for individual department specific data modeling requirements). Data is not kept at the aggregate level, but rather at the atomic level It is organized by business processes, and used by different departments Q: How do independent data marts differ from the Kimball bus structure? Answer : Every department has independent ETL processes & dimensional models These separate & smaller dimensional models are called data marts . There are different fact tables for the same events ( no conformed dimensions ) Q: What are the main characteristics of Inmon's corporate Information factory ( CIF )? Answer : It contains an enterprise data warehouse that provides a normalized data architecture before individual departments build on it The data marts use a source 3NF model and add denormalization based on department needs Q: Describe slicing (OLAP cube operation). Answer : It's reducing \\(N\\) dimensions to \\(N-1\\) dimensions by restricting one dimension to a single value. Example : By Original: InfopedianDerivative work: Jay - OLAP slicing.png, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=105680876 Q: Describe dicing (OLAP cube operation). Answer : It's computing a sub-cube by picking specific values of multiple dimensions. Example : By Original: InfopedianDerivative work: Jay - OLAP dicing.png, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=105725772 Q: Describe rolling-up (OLAP cube operation). Answer : It's aggregating or combining values and reducing the number of rows or columns. Q: Describe drilling-down (OLAP cube operation). Answer : It's decomposing values and increasing the number of rows or columns. Example : The picture shows a drill-down operation: The analyst moves from the summary category \"Outdoor-Schutzausr\u00fcstung\" to see the sales figures for the individual products. Von Infopedian - own illustration, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=14789910 Q: Describe the GROUP BY CUBE clause. Answer : PostgreSQL CUBE is a subclause of the GROUP BY clause. The CUBE allows you to generate multiple grouping sets. Example : SELECT c1, c2, c3, aggregate (c4) FROM table_name GROUP BY CUBE (c1, c2, c3); would generate all possible grouping sets based on the dimension columns specified in CUBE. It's a short way to define multiple grouping sets: CUBE(c1,c2,c3) GROUPING SETS ( (c1,c2,c3), (c1,c2), (c1,c3), (c2,c3), (c1), (c2), (c3), () )","title":"Introduction To Data Warehouses"},{"location":"knowledge/data_engineering_nanodegree_-_part_2_-_cloud_data_warehouses.html#module-2","text":"","title":"Module 2"},{"location":"knowledge/data_engineering_nanodegree_-_part_2_-_cloud_data_warehouses.html#introduction-to-cloud-computing-and-aws","text":"Q: What is cloud computing ? Answer : It is the practice of using a network of remote servers hosted on the internet to store , manage , and process data , rather than a local server or a personal computer. Q: What are advantages of cloud computing ? Answer : Eliminate need to invest in costly hardware upfront It's significantly faster provisioning the resources you need through the cloud versus the time it would take to gather and build up the hardware you'd need to provide the same support You can provide efficient access to your applications around the world by spreading your deployments to multiple regions. Q: In which three ways can AWS services be accessed? Answer : AWS Management Console , which is the web user interface. The AWS CLI is a useful way to control and automate your services with code SDK s allow you to easily integrate services with your applications through APIs built around specific languages and platforms.","title":"Introduction To Cloud Computing And Aws"},{"location":"knowledge/data_engineering_nanodegree_-_part_2_-_cloud_data_warehouses.html#module-3","text":"","title":"Module 3"},{"location":"knowledge/data_engineering_nanodegree_-_part_2_-_cloud_data_warehouses.html#implementing-data-warehouses-on-aws","text":"Q: Give the main technological characteristics of Redshift. Answer : It is column-oriented It is cloud-managed It provides massively parallel processing (MPP) Q: What is the number of nodes in a Redshift cluster equal to? Answer : The number of AWS EC2 instances used in the cluster. Q: What is each slice in a Redshift cluster? Answer : At least 1 CPU with dedicated storage and memory for the slice. Q: If we have a Redshift cluster with 4 nodes, each containing 8 slices, i.e. the cluster collectively offers 32 slices. What is the maximum number of partitions per table? Answer : 32 Remarks : The total number of slices in a cluster is our unit of parallelism and it is equal to the sum of all slices on the cluster. Q: In the ETL implementation on AWS, what is the purpose of the EC2 instance (ETL server)? Answer : It just acts as a client to RDS and Redshift to issue COPY commands Because we store all the data in S3 buckets, we do not need storage on the EC2 machine Q: What are advantages of using S3 for ETL storage? (Compared to storing the data in our own EC2 instance) Answer : S3 is AWS-managed, we don't need to worry about storage reliability. By using S3, we only pay for the storage we use, By using S3, we don't need to worry about not having enough storage. Q: Why do we might need to copy data already stored in S3 to another S3 staging bucket during the ETL process? Answer : Because we will most likely transform the data before inserting it into the DWH. Q: Which method is faster for ingesting data in a sql database? Using the INSERT command in a loop or using bulk insertion with the COPY command? Answer : Bulk insertion with the COPY command. Q: Why should we split a table into multiple files before ingesting them into Redshift? Answer : Because this way, we can execute multiple simultaneous COPY commands. Remarks : Each Redshift slice will act as a separate worker and will ingest the split of a file in parallel, so the process will complete much faster. Q: If you have multiple files that belong to the same table names as follows: s3://mySource/day1-sales.csv.gz s3://mySource/day2-sales.csv.gz ... Which method are you going to use to ingest the files from S3 into Redshift? Answer : You would need to create a manifest file. Remarks : Since the files have a common suffix and not a common prefix, we actually need to create a manifest specifying the list of files. If they were named: s3://mySource/sales-day1.csv.gz s3://mySource/sales-day2.csv.gz we could have relied on the existence of a common prefix. Q: What are advantages of infrastructure-as-code over creating infrastructure by clicking-around? Answer : Sharing : one can share all the steps with others easily Reproducibility : one can be sure that no steps are forgotten Multiple deployments : one can create a test environment identical to the production environment. Maintainability : if a change is needed, one can keep track of the changes by comparing code. Remarks : Sharing, Reproducibility, Multiple Deployments & Maintainability are all advantages of IaC. Q: What is Boto3? Answer : Boto3 is a Python SDK for programmatically accessing AWS. It enables developers to create, configure, and manage AWS services. Remarks : You can find the documentation for Boto3 here . Q: Which two strategies exist to optimize table design (slicing)? Answer : Distribution style Sorting Key Q: What is a drawback of joining 2 tables distributed using an EVEN strategy? Answer : It is slow because records need to be shuffled to put together the join result. Example : For example, a given key (say key=2532) of table 1 will not be on the same slice as the corresponding record in table 2, so the record will be copied (shuffled) between slices on different nodes, which results in slow performance. Q: What does the EVEN key distribution style mean? Answer : It means that a table is distributed across slices such that all slices have approximately the equal amount of records from the partitioned table. Q: What does the ALL distribution style do to a table? Answer : It replicates a table on all slices. Remarks : This is especially useful for small tables that are used often. Q: What's another common name for the ALL distribution style? Answer : \" Broadcasting \" is a common term used to the implementation of the ALL distribution style. Q: How does the AUTO distribution style work? Answer : It leaves the distribution decision to Redshift \"Small enough\" tables are distributed with an ALL strategy Large tables are distributed with an EVEN strategy Q: How does the key distribution style distribute the tables? Answer : It places rows with similar values on the same slice. Q: What are the four types of distribution styles in Redshift? Answer : Even All Auto Key","title":"Implementing Data Warehouses On Aws"},{"location":"knowledge/data_engineering_nanodegree_-_part_2_-_cloud_data_warehouses.html#acronyms","text":"DWH : Data Warehouse CIF : Corporate Information Factory ROLAP : Relational OLAP MOLAP : Multidimensional OLAP SDK : Software Development Kit","title":"Acronyms"},{"location":"knowledge/data_engineering_nanodegree_-_part_3_-_data_lakes_with_spark.html","text":"Data Engineering Nanodegree - Part 3 - Data Lakes with Spark \u00b6 Data Engineering \u00b6 Data Lakes With Spark \u00b6 Module 4 \u00b6 Data Wrangling With Spark \u00b6 Q: In which language is Spark written? Answer : Scala Q: How is it possible that Spark programs can be written in Python if Python is not a functional programming language? Answer : The PySpark API allows you to write programs in Spark and ensures that your code uses functional programming practices. Underneath the hood, the Python code uses py4j to make calls to the Java Virtual Machine (JVM). Q: What are resilient distributed datasets ( RDD 's)? Answer : RDDs are exactly what they say they are: fault-tolerant datasets distributed across a cluster. This is how Spark stores data. The Power Of Spark \u00b6 Q: What's the CPU? Answer : The CPU is the \"brain\" of the computer. Remarks : Every process on your computer is eventually handled by your CPU. This includes calculations and also instructions for the other components of the compute. Q: What's the memory (RAM)? Answer : When your program runs, data gets temporarily stored in memory before getting sent to the CPU. Memory is ephemeral storage - when your computer shuts down, the data in the memory is lost. Q: What's the storage (SSD or Magnetic Disk)? Answer : Storage is used for keeping data over long periods of time. When a program runs, the CPU will direct the memory to temporarily load data from long-term storage. Q: What's the Network (LAN or Internet)? Answer : Network is the gateway for anything that you need that isn't stored on your computer. The network could connect to other computers in the same room (a Local Area Network) or to a computer on the other side of the world, connected over the internet. Q: Rank the following hardware components in order from fastests to slowest: Memory, Disk Storage, Network, CPU. Answer : CPU Memory (RAM) Disk Storage (SSD) Network Remarks : CPU operations are fastest. Operations in memory (RAM) are the second fastest. Then comes hard disk storage and finally transferring data across a network. Keep these relative speeds in mind. They'll help you understand the constraints when working with big data. Q: What are the functions of the CPU ? Answer : It has a few different functions including directing other components of a computer as well as running mathematical calculations. The CPU can also store small amounts of data inside itself in what are called registers . Example : For example, say you write a program that reads in a 40 MB data file and then analyzes the file. When you execute the code, the instructions are loaded into the CPU. The CPU then instructs the computer to take the 40 MB from disk and store the data in memory (RAM). If you want to sum a column of data, then the CPU will essentially take two numbers at a time and sum them together. The accumulation of the sum needs to be stored somewhere while the CPU grabs the next number. This cumulative sum will be stored in a register. The registers make computations more efficient: the registers avoid having to send data unnecessarily back and forth between memory (RAM) and the CPU. Q: What does it mean for a CPU to be 2.5 Gigahertz? Answer : It means that the CPU processes 2.5 billion operations per second. Q: Knowing that tweets create approximately 104 billion bytes of data per day, how long would it take the 2.5 GigaHertz CPU to analyze a full day of tweets? Answer : 104 billion bytes * (1 second / 20 billion bytes) = 5.2 seconds Remarks : Twitter generates about 6,000 tweets per second, and each tweet contains 200 bytes. So in one day, Twitter generates data on the order of: (6000 tweets / second) x (86400 seconds / day) x (200 bytes / tweet) = 104 billion bytes / day Q: What are the limitations of memory (RAM)? Answer : It's relatively expensive It's ephemeral (data stored in RAM gets erased when the computer shuts down) Remarks : However, it is efficient: operations in RAM are relatively fast compared to reading and writing from disk or moving data across a network. Q: What is shuffling ? Answer : Moving data back and forth between different nodes of a cluster. Remarks : Since this is very time expensive, Spark tries to reduce shuffling. Q: List the key ratios of processing speed between the major hardware components. Answer : CPU: 200x faster than memory Memory: 15x faster than SSD SSD: 20x faster than network Q: What's a difference between parallel computing and distributed computing ? Answer : At a high level, distributed computing implies multiple CPUs each with its own memory. Parallel computing uses multiple CPUs sharing the same memory. Q: What are the four components of Hadoop ? Answer : Hadoop - an ecosystem of tools for big data storage and data analysis. Hadoop is an older system than Spark but is still used by many companies. Hadoop MapReduce - a system for processing and analyzing large data sets in parallel. Hadoop YARN - a resource manager that schedules jobs across a cluster. The manager keeps track of what computer resources are available and then assigns those resources to specific tasks. Hadoop Distributed File System (HDFS) - a big data storage system that splits data into chunks and stores the chunks across a cluster of computers. Remarks : The major difference between Spark and Hadoop is how they use memory. Hadoop writes intermediate results to disk whereas Spark tries to keep data in memory whenever possible. This makes Spark faster for many use cases. Q: How does Spark differ from Hadoop? Answer : Spark is generally faster than Hadoop. This is because Hadoop writes intermediate results to disk whereas Spark tries to keep intermediate results in memory whenever possible. The Hadoop ecosystem includes a distributed file storage system called HDFS (Hadoop Distributed File System). Spark, on the other hand, does not include a file storage system. You can use Spark on top of HDFS but you do not have to. Spark can read in data from other sources as well such as Amazon S3. Q: What is MapReduce? Answer : MapReduce is a programming technique for manipulating large data sets. Remarks : \"Hadoop MapReduce\" is a specific implementation of this programming technique. Q: How does MapReduce work? Answer : The technique works by first dividing up a large dataset and distributing the data across a cluster. In the map step , each data is analyzed and converted into a (key, value) pair . Then these key-value pairs are shuffled across the cluster so that all keys are on the same machine. In the reduce step , the values with the same keys are combined together. Q: What happens in the shuffle step of MapReduce? Answer : The shuffle step finds all of the data across the clusters that have the same key. And all of those data points with that key are brought into the same network node for further analysis. Q: What are the four modes to set up Spark? Answer : Local Spark standalone YARN Mesos Module 5 \u00b6 Introduction To Data Lakes \u00b6 Q: List differences between Data Warehouses and Data Lakes Answer : Data form : tabular vs. all formats Data value : high only vs. high- or medium-value, or to be discoverd Ingestion : ETL vs. ELT Data model : Star & Snowflake with conformed dimensions or data-marts and OLAP cubes vs. Star & Snowflakes and OLAP are also possible but other ad-hoc represenations are possible Schema : Known before ingestion (schema-on-write) vs. On-the-fly at the time of analysis (schema-on-read) Technology : Expensive MPP databases with expensive disks and connectivity vs. Commodity hardware with parallelism as first principle. Data Quality : High with effort for consistency and clear rules for accessibility vs. mixed, some data remain in raw format, some data is transformed to higher quality Users : Busines analysts vs. Data scientists, business analysts & ML engineers Analytics : Reports and business intelligence visualizations vs. Machine Learning, graph analytics and data exploration. Q: What is schema-on-read ? Answer : Schema on read refers to an innovative data analysis strategy in new data-handling tools like Hadoop and other more involved database technologies. In schema on read, data is applied to a plan or schema as it is pulled out of a stored location, rather than as it goes in. Q: Difference between data warehouses and data lakes Answer : Characteristics Data Warehouse Data Lake Data Relational from transactional systems, operational databases, and line of business applications Non-relational and relational from IoT devices, web sites, mobile apps, social media, and corporate applications Schema Designed prior to the DW implementation (schema-on-write) Written at the time of analysis (schema-on-read) Price/Performance Fastest query results using higher cost storage Query results getting faster using low-cost storage Data Quality Highly curated data that serves as the central version of the truth Any data that may or may not be curated (ie. raw data) Users Business analysts Data scientists, Data developers, and Business analysts (using curated data) Analytics Batch reporting, BI and visualizations Machine Learning, Predictive analytics, data discovery and profiling Q: What is a data lake ? Answer : A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics\u2014from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions. Example : Q: Describe the bottled water vs. data lake analogy Answer : A data warehouse is like a producer of water where you are handed bottled water in a particular size and shape In contrast, a data lake is a lake where many water streams flow into it and everyone is free to choose the water in the way they want to. Q: What are issues of data lakes ? Answer : Data lakes are prone to being a chaotic data garbage dump (\"Datensumpf). To prevent this, detailed metadata (e.g. a data catalog) should be put in place. Acronyms \u00b6 RDD : Resilient Distributed Dataset JVM : Java Virtual Machine CPU : Central Processing Unit RAM : Random Access Memory SSD : Solid State Drive LAN : Local Area Network HDFS : Hadoop Distributed File System","title":"Data Engineering Nanodegree - Part 3 - Data Lakes with Spark"},{"location":"knowledge/data_engineering_nanodegree_-_part_3_-_data_lakes_with_spark.html#data-engineering-nanodegree-part-3-data-lakes-with-spark","text":"","title":"Data Engineering Nanodegree - Part 3 - Data Lakes with Spark"},{"location":"knowledge/data_engineering_nanodegree_-_part_3_-_data_lakes_with_spark.html#data-engineering","text":"","title":"Data Engineering"},{"location":"knowledge/data_engineering_nanodegree_-_part_3_-_data_lakes_with_spark.html#data-lakes-with-spark","text":"","title":"Data Lakes With Spark"},{"location":"knowledge/data_engineering_nanodegree_-_part_3_-_data_lakes_with_spark.html#module-4","text":"","title":"Module 4"},{"location":"knowledge/data_engineering_nanodegree_-_part_3_-_data_lakes_with_spark.html#data-wrangling-with-spark","text":"Q: In which language is Spark written? Answer : Scala Q: How is it possible that Spark programs can be written in Python if Python is not a functional programming language? Answer : The PySpark API allows you to write programs in Spark and ensures that your code uses functional programming practices. Underneath the hood, the Python code uses py4j to make calls to the Java Virtual Machine (JVM). Q: What are resilient distributed datasets ( RDD 's)? Answer : RDDs are exactly what they say they are: fault-tolerant datasets distributed across a cluster. This is how Spark stores data.","title":"Data Wrangling With Spark"},{"location":"knowledge/data_engineering_nanodegree_-_part_3_-_data_lakes_with_spark.html#the-power-of-spark","text":"Q: What's the CPU? Answer : The CPU is the \"brain\" of the computer. Remarks : Every process on your computer is eventually handled by your CPU. This includes calculations and also instructions for the other components of the compute. Q: What's the memory (RAM)? Answer : When your program runs, data gets temporarily stored in memory before getting sent to the CPU. Memory is ephemeral storage - when your computer shuts down, the data in the memory is lost. Q: What's the storage (SSD or Magnetic Disk)? Answer : Storage is used for keeping data over long periods of time. When a program runs, the CPU will direct the memory to temporarily load data from long-term storage. Q: What's the Network (LAN or Internet)? Answer : Network is the gateway for anything that you need that isn't stored on your computer. The network could connect to other computers in the same room (a Local Area Network) or to a computer on the other side of the world, connected over the internet. Q: Rank the following hardware components in order from fastests to slowest: Memory, Disk Storage, Network, CPU. Answer : CPU Memory (RAM) Disk Storage (SSD) Network Remarks : CPU operations are fastest. Operations in memory (RAM) are the second fastest. Then comes hard disk storage and finally transferring data across a network. Keep these relative speeds in mind. They'll help you understand the constraints when working with big data. Q: What are the functions of the CPU ? Answer : It has a few different functions including directing other components of a computer as well as running mathematical calculations. The CPU can also store small amounts of data inside itself in what are called registers . Example : For example, say you write a program that reads in a 40 MB data file and then analyzes the file. When you execute the code, the instructions are loaded into the CPU. The CPU then instructs the computer to take the 40 MB from disk and store the data in memory (RAM). If you want to sum a column of data, then the CPU will essentially take two numbers at a time and sum them together. The accumulation of the sum needs to be stored somewhere while the CPU grabs the next number. This cumulative sum will be stored in a register. The registers make computations more efficient: the registers avoid having to send data unnecessarily back and forth between memory (RAM) and the CPU. Q: What does it mean for a CPU to be 2.5 Gigahertz? Answer : It means that the CPU processes 2.5 billion operations per second. Q: Knowing that tweets create approximately 104 billion bytes of data per day, how long would it take the 2.5 GigaHertz CPU to analyze a full day of tweets? Answer : 104 billion bytes * (1 second / 20 billion bytes) = 5.2 seconds Remarks : Twitter generates about 6,000 tweets per second, and each tweet contains 200 bytes. So in one day, Twitter generates data on the order of: (6000 tweets / second) x (86400 seconds / day) x (200 bytes / tweet) = 104 billion bytes / day Q: What are the limitations of memory (RAM)? Answer : It's relatively expensive It's ephemeral (data stored in RAM gets erased when the computer shuts down) Remarks : However, it is efficient: operations in RAM are relatively fast compared to reading and writing from disk or moving data across a network. Q: What is shuffling ? Answer : Moving data back and forth between different nodes of a cluster. Remarks : Since this is very time expensive, Spark tries to reduce shuffling. Q: List the key ratios of processing speed between the major hardware components. Answer : CPU: 200x faster than memory Memory: 15x faster than SSD SSD: 20x faster than network Q: What's a difference between parallel computing and distributed computing ? Answer : At a high level, distributed computing implies multiple CPUs each with its own memory. Parallel computing uses multiple CPUs sharing the same memory. Q: What are the four components of Hadoop ? Answer : Hadoop - an ecosystem of tools for big data storage and data analysis. Hadoop is an older system than Spark but is still used by many companies. Hadoop MapReduce - a system for processing and analyzing large data sets in parallel. Hadoop YARN - a resource manager that schedules jobs across a cluster. The manager keeps track of what computer resources are available and then assigns those resources to specific tasks. Hadoop Distributed File System (HDFS) - a big data storage system that splits data into chunks and stores the chunks across a cluster of computers. Remarks : The major difference between Spark and Hadoop is how they use memory. Hadoop writes intermediate results to disk whereas Spark tries to keep data in memory whenever possible. This makes Spark faster for many use cases. Q: How does Spark differ from Hadoop? Answer : Spark is generally faster than Hadoop. This is because Hadoop writes intermediate results to disk whereas Spark tries to keep intermediate results in memory whenever possible. The Hadoop ecosystem includes a distributed file storage system called HDFS (Hadoop Distributed File System). Spark, on the other hand, does not include a file storage system. You can use Spark on top of HDFS but you do not have to. Spark can read in data from other sources as well such as Amazon S3. Q: What is MapReduce? Answer : MapReduce is a programming technique for manipulating large data sets. Remarks : \"Hadoop MapReduce\" is a specific implementation of this programming technique. Q: How does MapReduce work? Answer : The technique works by first dividing up a large dataset and distributing the data across a cluster. In the map step , each data is analyzed and converted into a (key, value) pair . Then these key-value pairs are shuffled across the cluster so that all keys are on the same machine. In the reduce step , the values with the same keys are combined together. Q: What happens in the shuffle step of MapReduce? Answer : The shuffle step finds all of the data across the clusters that have the same key. And all of those data points with that key are brought into the same network node for further analysis. Q: What are the four modes to set up Spark? Answer : Local Spark standalone YARN Mesos","title":"The Power Of Spark"},{"location":"knowledge/data_engineering_nanodegree_-_part_3_-_data_lakes_with_spark.html#module-5","text":"","title":"Module 5"},{"location":"knowledge/data_engineering_nanodegree_-_part_3_-_data_lakes_with_spark.html#introduction-to-data-lakes","text":"Q: List differences between Data Warehouses and Data Lakes Answer : Data form : tabular vs. all formats Data value : high only vs. high- or medium-value, or to be discoverd Ingestion : ETL vs. ELT Data model : Star & Snowflake with conformed dimensions or data-marts and OLAP cubes vs. Star & Snowflakes and OLAP are also possible but other ad-hoc represenations are possible Schema : Known before ingestion (schema-on-write) vs. On-the-fly at the time of analysis (schema-on-read) Technology : Expensive MPP databases with expensive disks and connectivity vs. Commodity hardware with parallelism as first principle. Data Quality : High with effort for consistency and clear rules for accessibility vs. mixed, some data remain in raw format, some data is transformed to higher quality Users : Busines analysts vs. Data scientists, business analysts & ML engineers Analytics : Reports and business intelligence visualizations vs. Machine Learning, graph analytics and data exploration. Q: What is schema-on-read ? Answer : Schema on read refers to an innovative data analysis strategy in new data-handling tools like Hadoop and other more involved database technologies. In schema on read, data is applied to a plan or schema as it is pulled out of a stored location, rather than as it goes in. Q: Difference between data warehouses and data lakes Answer : Characteristics Data Warehouse Data Lake Data Relational from transactional systems, operational databases, and line of business applications Non-relational and relational from IoT devices, web sites, mobile apps, social media, and corporate applications Schema Designed prior to the DW implementation (schema-on-write) Written at the time of analysis (schema-on-read) Price/Performance Fastest query results using higher cost storage Query results getting faster using low-cost storage Data Quality Highly curated data that serves as the central version of the truth Any data that may or may not be curated (ie. raw data) Users Business analysts Data scientists, Data developers, and Business analysts (using curated data) Analytics Batch reporting, BI and visualizations Machine Learning, Predictive analytics, data discovery and profiling Q: What is a data lake ? Answer : A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics\u2014from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions. Example : Q: Describe the bottled water vs. data lake analogy Answer : A data warehouse is like a producer of water where you are handed bottled water in a particular size and shape In contrast, a data lake is a lake where many water streams flow into it and everyone is free to choose the water in the way they want to. Q: What are issues of data lakes ? Answer : Data lakes are prone to being a chaotic data garbage dump (\"Datensumpf). To prevent this, detailed metadata (e.g. a data catalog) should be put in place.","title":"Introduction To Data Lakes"},{"location":"knowledge/data_engineering_nanodegree_-_part_3_-_data_lakes_with_spark.html#acronyms","text":"RDD : Resilient Distributed Dataset JVM : Java Virtual Machine CPU : Central Processing Unit RAM : Random Access Memory SSD : Solid State Drive LAN : Local Area Network HDFS : Hadoop Distributed File System","title":"Acronyms"},{"location":"knowledge/data_engineering_nanodegree_-_part_4_-_data_pipelines_with_airflow.html","text":"Data Engineering Nanodegree - Part 4 - Data Pipelines With Airflow \u00b6 Data Engineering \u00b6 Data Pipelines With Airflow \u00b6 Module 1 \u00b6 Data Pipelines \u00b6 Q: What is a data pipeline ? Answer : A series of steps in which data is processed. Example : Automated marketing emails Real-time pricing in rideshare apps Targeted advertising based on browsing history Remarks : It's typically using either ETL or ELT. Q: Given an example of a data pipeline to accomplish the following: a bikeshare company wants to efficiently improve availability of bikes. Answer : Load application event data from a source such as S3 or Kafka Load the data into an analytic warehouse such as RedShift. Perform data transformations that identify high-traffic bike docks so the business can determine where to build additional locations. Q: Describe the difference between ETL and ELT. Answer : ETL is normally a continuous, ongoing process with a well-defined workflow. ETL first extracts data from homogeneous or heterogeneous data sources. Then, data is cleansed, enriched, transformed, and stored either back in the lake or in a data warehouse. ELT (Extract, Load, Transform) is a variant of ETL wherein the extracted data is first loaded into the target system. Transformations are performed after the data is loaded into the data warehouse. Remarks : ELT typically works well when the target system is powerful enough to handle transformations. Analytical databases like Amazon Redshift and Google BigQ. Q: What is S3 ? Answer : Amazon S3 has a simple web services interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. It gives any developer access to the same highly scalable, reliable, fast, inexpensive data storage infrastructure that Amazon uses to run its own global network of web sites. Remarks : Source: Amazon Web Services Documentation . If you want to learn more, start here . Q: What is Kafka ? Answer : Apache Kafka is an open-source stream-processing software platform developed by Linkedin and donated to the Apache Software Foundation, written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Remarks : Its storage layer is essentially a massively scalable pub/sub message queue designed as a distributed transaction log, making it highly valuable for enterprise infrastructures to process streaming data.\" Source: Wikipedia If you want to learn more, start here . Q: What is RedShift ? Answer : Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. Remarks : The first step to create a data warehouse is to launch a set of nodes, called an Amazon Redshift cluster. After you provision your cluster, you can upload your data set and then perform data analysis queries. Regardless of the size of the data set, Amazon Redshift offers fast query performance using the same SQL-based tools and business intelligence applications that you use today. If you want to learn more, start here . Q: What is data validation ? Answer : It is the process of ensuring that data is present, correct & meaningful. Remarks : Ensuring the quality of your data through automated validation checks is a critical step in building data pipelines at any organization. Q: List some data validation steps for the bikeshare example. Answer : After loading from S3 to Redshift: Validate the number of rows in Redshift match the number of records in S3 Once location business analysis is complete: Validate that all locations have a daily visit average greater than 0 Validate that the number of locations in our output table match the number of tables in the input table. Q: Are there real world cases where a data pipeline is not a DAG? Answer : It is possible to model a data pipeline that is not a DAG, meaning that it contains a cycle within the process. However, the vast majority of use cases for data pipelines can be described as a directed acyclic graph (DAG). This makes the code more understandable and maintainable. Q: Can we have two different pipelines for the same data and can we merge them back together? Answer : Yes. It's not uncommon for a data pipeline to take the same dataset, perform two different processes to analyze it, then merge the results of those two processes back together. Q: How could the bikeshare example be modeled as a DAG? Answer : Q: What are the five components of airflow ? Answer : The s cheduler orchestrates the execution of jobs on a trigger or schedule. It chooses how to prioritize the running and execution of tasks within the system. The work queue is used by the scheduler in most Airflow installations to deliver tasks that need to be run to the workers. Worker processes execute the operations defined in each DAG. In most Airflow installations, workers pull from the work queue when it is ready to process a task. When the worker completes the execution of the task, it will attempt to process more work from the work queue until there is no further work remaining. Database saves credentials, connections, history, and configuration. The database, often referred to as the metadata database , also stores the state of all tasks in the system. The web Interface provides a control dashboard for users and maintainers. Throughout this course you will see how the web interface allows users to perform tasks such as stopping and starting DAGs, retrying failed tasks, configuring credentials, Example : Q: Describe the order of operations for an Airflow DAG. Answer : The scheduler starts DAGs based on time or external triggers. Once a DAG is started, the scheduler looks at the steps within the DAG and determines which steps can run by looking at their dependencies. The scheduler places runnable steps in the queue. Workers pick up those tasks and run them. Once the worker has finished running the step, the final status of the task is recorded and additional tasks are placed by the scheduler until all tasks are complete. Once all tasks have been completed, the DAG is complete. Example : Q: What does the airflow scheduler do? Answer : It starts DAGs based on triggers or schedules and moves them towards completion. Q: What do the Airflow workers do? Answer : They run and record the outcome of individual pipeline tasks. Q: How to create a DAG in Airflow? Answer : Creating a DAG is easy. Give it a name , a description , a start date , and an interval . Example : Q: What are operators in Airflow? Answer : They define the atomic steps of work that make up a DAG. Example : Remarks : Instantiated operators are referred to as Tasks . Q: What does Airflow do when the start date is in the past? Answer : Airflow will run your DAG as many times as there are schedule intervals between that start date and the current date. Q: What are the nodes and edges in an Airflow DAG? Answer : Nodes = Tasks Edges = Ordering and dependencies between tasks Remarks : Task dependencies can be described programmatically in Airflow using >> and << a >> b means a comes before b a << b means a comes after b Q: How to programmatically describe task dependencies in Airflow? Answer : Task dependencies can be described programmatically in Airflow using >> and << a >> b means a comes before b a << b means a comes after b Example : ``` hello_world_task = PythonOperator(task_id=\u2019hello_world\u2019, ...) goodbye_world_task = PythonOperator(task_id=\u2019goodbye_world\u2019, ...) ``` Use `>>` to denote that `goodbye_world_task` depends on `hello_world_task` ```hello_world_task >> goodbye_world_task``` Q: What are Airflow Hooks? Answer : Hooks provide a reusable interface to external systems and databases. Example : Remarks : With hooks, you don\u2019t have to worry about how and where to store these connection strings and secrets in your code. Q: What's the purpose of runtime variables in Airflow? Answer : They allow users to \u201cfill in the blank\u201d with important runtime variables for tasks. Example : Remarks : Here is the Apache Airflow documentation on context variables that can be included as kwargs. Q: What is a Task in Airflow? Answer : An instantiated step in a pipeline fully parameterized for execution. Module 2 \u00b6 Data Quality \u00b6 Q: What is data lineage ? Answer : The data lineage of a dataset describes the discrete steps involved in the creation, movement, and calculation of that dataset. Q: Why is data lineage important? Answer : Instilling Confidence: Being able to describe the data lineage of a particular dataset or analysis will build confidence in data consumers (engineers, analysts, data scientists, etc.) that our data pipeline is creating meaningful results using the correct datasets. Defining Metrics: Another major benefit of surfacing data lineage is that it allows everyone in the organization to agree on the definition of how a particular metric is calculated. Debugging: Data lineage helps data engineers track down the root of errors when they occur. If each step of the data movement and transformation process is well described, it's easy to find problems when they occur. Remarks : In general, data lineage has important implications for a business. Each department or business unit's success is tied to data and to the flow of data between departments. For e.g., sales departments rely on data to make sales forecasts, while at the same time the finance department would need to track sales and revenue. Each of these departments and roles depend on data, and knowing where to find the data. Data flow and data lineage tools enable data engineers and architects to track the flow of this large web of data. Q: Why use schedules in airflow? Answer : Pipeline schedules can reduce the amount of data that needs to be processed in a given run. It helps scope the job to only run the data for the time period since the data pipeline last ran. Using schedules to select only data relevant to the time period of the given pipeline execution can help improve the quality and accuracy of the analyses performed by our pipeline. Running pipelines on a schedule will decrease the time it takes the pipeline to run. An analysis of larger scope can leverage already-completed work . For example, if the aggregates for all months prior to now have already been done by a scheduled job, then we only need to perform the aggregation for the current month and add it to the existing totals. Q: Which factors should be considered in selecting a time period for scheduling? Answer : What is the size of data, on average, for a time period? If an entire years worth of data is only a few kb or mb, then perhaps its fine to load the entire dataset. If an hours worth of data is hundreds of mb or even in the gbs then likely you will need to schedule your pipeline more frequently. How frequently is data arriving, and how often does the analysis need to be performed? If our bikeshare company needs trip data every hour, that will be a driving factor in determining the schedule. Alternatively, if we have to load hundreds of thousands of tiny records, even if they don't add up to much in terms of mb or gb, the file access alone will slow down our analysis and we\u2019ll likely want to run it more often. What's the frequency on related datasets? A good rule of thumb is that the frequency of a pipeline\u2019s schedule should be determined by the dataset in our pipeline which requires the most frequent analysis. This isn\u2019t universally the case, but it's a good starting assumption. For example, if our trips data is updating every hour, but our bikeshare station table only updates once a quarter, we\u2019ll probably want to run our trip analysis every hour, and not once a quarter. Q: What is the scope of a data pipeline? Answer : It is the time delta between the current execution time and the end time of the last execution. Q: What is data partitioning ? Answer : It is the process of isolating data to be analyzed by one or more attributes, such as time, logical type, or data size. Remarks : It often leads to faster and more reliable pipelines. Q: Why should you use data partitioning ? Answer : Pipelines designed to work with partitioned data fail more gracefully . Smaller datasets, smaller time periods, and related concepts are easier to debug than big datasets, large time periods, and unrelated concepts. Partitioning makes rerunning failed tasks much simpler . It also enables easier redos of work, reducing cost and time. Remarks : Another great thing about Airflow is that if your data is partitioned appropriately, your tasks will naturally have fewer dependencies on each other. Because of this, Airflow will be able to parallelize execution of your DAGs to produce your results even faster. Q: What are four common types of data partitioning? Answer : Location Logical Size Time Q: What is logical partitioning ? Answer : The process of breaking conceptually related data into discrete groups for processing. Q: What is time partitioning ? Answer : It is the process of processing data based on a schedule or when it was created. Q: What is size partitioning ? Answer : It is the process of separating data for processing based on desired or required storage limits. Q: How would you set a requirement for ensuring that data arrives within a certain timeframe of a DAG starting? Answer : Using a service level agreement (SLA). Module 3 \u00b6 Production Data Pipelines \u00b6 Q: What are the most common types of user-created plugins? Answer : Operators and Hooks. Q: How to create a custom operator? Answer : Identify Operators that perform similar functions and can be consolidated Define a new Operator in the plugins folder Replace the original Operators with your new custom one, re-parameterize, and instantiate them. Q: What is Airflow contrib and why is it useful? Answer : It contains Operators and hooks for common data tools like Apache Spark and Cassandra, as well as vendor specific integrations for Amazon Web Services, Azure, and Google Cloud Platform can be found in Airflow contrib. Therefore you should always check it before building your own airflow plugins to see if what you need already exists. Remarks : If the functionality exists and its not quite what you want, that\u2019s a great opportunity to add that functionality through an open source contribution. Q: Which three rules should you follow when designing DAGs? Answer : DAG tasks should be designed such that they: are Atomic and have a single purpose Maximize parallelism Make failure states obvious Remarks : Every task in your dag should perform only one job. \u201cWrite programs that do one thing and do it well.\u201d - Ken Thompson\u2019s Unix Philosophy Q: What are the benefits of task boundaries ? Answer : Re-visitable: Task boundaries are useful for you if you revisit a pipeline you wrote after a 6 month absence. You'll have a much easier time understanding how it works and the lineage of the data if the boundaries between tasks are clear and well defined. This is true in the code itself, and within the Airflow UI. Tasks that do just one thing are often more easily parallelized . This parallelization can offer a significant speedup in the execution of our DAGs. Q: What are benefits of SubDAGs ? Answer : Decrease the amount of code we need to write and maintain to create a new DAG Easier to understand the high level goals of a DAG Bug fixes, speedups, and other enhancements can be made more quickly and distributed to all DAGs that use that SubDAG Q: What are drawbacks of using SubDAGs? Answer : Limit the visibility within the Airflow UI Abstraction makes understanding what the DAG is doing more difficult Encourages premature optimization Q: Which pipeline monitoring options does airflow provide? Answer : SLAs Emails and alerts Metrics Q: What's the purpose of SLAs in airflow? Answer : SLAs define the time by which a DAG must complete. For time-sensitive applications they are critical for developing trust amongst your pipeline customers and ensuring that data is delivered while it is still meaningful. Slipping SLAs can also be early indicators of performance problems , or a need to scale up the size of your Airflow cluster Acronyms \u00b6 SLA : Service Level Agreement DAG : Directed Acyclic Graph","title":"Data Engineering Nanodegree - Part 4 - Data Pipelines With Airflow"},{"location":"knowledge/data_engineering_nanodegree_-_part_4_-_data_pipelines_with_airflow.html#data-engineering-nanodegree-part-4-data-pipelines-with-airflow","text":"","title":"Data Engineering Nanodegree - Part 4 - Data Pipelines With Airflow"},{"location":"knowledge/data_engineering_nanodegree_-_part_4_-_data_pipelines_with_airflow.html#data-engineering","text":"","title":"Data Engineering"},{"location":"knowledge/data_engineering_nanodegree_-_part_4_-_data_pipelines_with_airflow.html#data-pipelines-with-airflow","text":"","title":"Data Pipelines With Airflow"},{"location":"knowledge/data_engineering_nanodegree_-_part_4_-_data_pipelines_with_airflow.html#module-1","text":"","title":"Module 1"},{"location":"knowledge/data_engineering_nanodegree_-_part_4_-_data_pipelines_with_airflow.html#data-pipelines","text":"Q: What is a data pipeline ? Answer : A series of steps in which data is processed. Example : Automated marketing emails Real-time pricing in rideshare apps Targeted advertising based on browsing history Remarks : It's typically using either ETL or ELT. Q: Given an example of a data pipeline to accomplish the following: a bikeshare company wants to efficiently improve availability of bikes. Answer : Load application event data from a source such as S3 or Kafka Load the data into an analytic warehouse such as RedShift. Perform data transformations that identify high-traffic bike docks so the business can determine where to build additional locations. Q: Describe the difference between ETL and ELT. Answer : ETL is normally a continuous, ongoing process with a well-defined workflow. ETL first extracts data from homogeneous or heterogeneous data sources. Then, data is cleansed, enriched, transformed, and stored either back in the lake or in a data warehouse. ELT (Extract, Load, Transform) is a variant of ETL wherein the extracted data is first loaded into the target system. Transformations are performed after the data is loaded into the data warehouse. Remarks : ELT typically works well when the target system is powerful enough to handle transformations. Analytical databases like Amazon Redshift and Google BigQ. Q: What is S3 ? Answer : Amazon S3 has a simple web services interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. It gives any developer access to the same highly scalable, reliable, fast, inexpensive data storage infrastructure that Amazon uses to run its own global network of web sites. Remarks : Source: Amazon Web Services Documentation . If you want to learn more, start here . Q: What is Kafka ? Answer : Apache Kafka is an open-source stream-processing software platform developed by Linkedin and donated to the Apache Software Foundation, written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Remarks : Its storage layer is essentially a massively scalable pub/sub message queue designed as a distributed transaction log, making it highly valuable for enterprise infrastructures to process streaming data.\" Source: Wikipedia If you want to learn more, start here . Q: What is RedShift ? Answer : Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. Remarks : The first step to create a data warehouse is to launch a set of nodes, called an Amazon Redshift cluster. After you provision your cluster, you can upload your data set and then perform data analysis queries. Regardless of the size of the data set, Amazon Redshift offers fast query performance using the same SQL-based tools and business intelligence applications that you use today. If you want to learn more, start here . Q: What is data validation ? Answer : It is the process of ensuring that data is present, correct & meaningful. Remarks : Ensuring the quality of your data through automated validation checks is a critical step in building data pipelines at any organization. Q: List some data validation steps for the bikeshare example. Answer : After loading from S3 to Redshift: Validate the number of rows in Redshift match the number of records in S3 Once location business analysis is complete: Validate that all locations have a daily visit average greater than 0 Validate that the number of locations in our output table match the number of tables in the input table. Q: Are there real world cases where a data pipeline is not a DAG? Answer : It is possible to model a data pipeline that is not a DAG, meaning that it contains a cycle within the process. However, the vast majority of use cases for data pipelines can be described as a directed acyclic graph (DAG). This makes the code more understandable and maintainable. Q: Can we have two different pipelines for the same data and can we merge them back together? Answer : Yes. It's not uncommon for a data pipeline to take the same dataset, perform two different processes to analyze it, then merge the results of those two processes back together. Q: How could the bikeshare example be modeled as a DAG? Answer : Q: What are the five components of airflow ? Answer : The s cheduler orchestrates the execution of jobs on a trigger or schedule. It chooses how to prioritize the running and execution of tasks within the system. The work queue is used by the scheduler in most Airflow installations to deliver tasks that need to be run to the workers. Worker processes execute the operations defined in each DAG. In most Airflow installations, workers pull from the work queue when it is ready to process a task. When the worker completes the execution of the task, it will attempt to process more work from the work queue until there is no further work remaining. Database saves credentials, connections, history, and configuration. The database, often referred to as the metadata database , also stores the state of all tasks in the system. The web Interface provides a control dashboard for users and maintainers. Throughout this course you will see how the web interface allows users to perform tasks such as stopping and starting DAGs, retrying failed tasks, configuring credentials, Example : Q: Describe the order of operations for an Airflow DAG. Answer : The scheduler starts DAGs based on time or external triggers. Once a DAG is started, the scheduler looks at the steps within the DAG and determines which steps can run by looking at their dependencies. The scheduler places runnable steps in the queue. Workers pick up those tasks and run them. Once the worker has finished running the step, the final status of the task is recorded and additional tasks are placed by the scheduler until all tasks are complete. Once all tasks have been completed, the DAG is complete. Example : Q: What does the airflow scheduler do? Answer : It starts DAGs based on triggers or schedules and moves them towards completion. Q: What do the Airflow workers do? Answer : They run and record the outcome of individual pipeline tasks. Q: How to create a DAG in Airflow? Answer : Creating a DAG is easy. Give it a name , a description , a start date , and an interval . Example : Q: What are operators in Airflow? Answer : They define the atomic steps of work that make up a DAG. Example : Remarks : Instantiated operators are referred to as Tasks . Q: What does Airflow do when the start date is in the past? Answer : Airflow will run your DAG as many times as there are schedule intervals between that start date and the current date. Q: What are the nodes and edges in an Airflow DAG? Answer : Nodes = Tasks Edges = Ordering and dependencies between tasks Remarks : Task dependencies can be described programmatically in Airflow using >> and << a >> b means a comes before b a << b means a comes after b Q: How to programmatically describe task dependencies in Airflow? Answer : Task dependencies can be described programmatically in Airflow using >> and << a >> b means a comes before b a << b means a comes after b Example : ``` hello_world_task = PythonOperator(task_id=\u2019hello_world\u2019, ...) goodbye_world_task = PythonOperator(task_id=\u2019goodbye_world\u2019, ...) ``` Use `>>` to denote that `goodbye_world_task` depends on `hello_world_task` ```hello_world_task >> goodbye_world_task``` Q: What are Airflow Hooks? Answer : Hooks provide a reusable interface to external systems and databases. Example : Remarks : With hooks, you don\u2019t have to worry about how and where to store these connection strings and secrets in your code. Q: What's the purpose of runtime variables in Airflow? Answer : They allow users to \u201cfill in the blank\u201d with important runtime variables for tasks. Example : Remarks : Here is the Apache Airflow documentation on context variables that can be included as kwargs. Q: What is a Task in Airflow? Answer : An instantiated step in a pipeline fully parameterized for execution.","title":"Data Pipelines"},{"location":"knowledge/data_engineering_nanodegree_-_part_4_-_data_pipelines_with_airflow.html#module-2","text":"","title":"Module 2"},{"location":"knowledge/data_engineering_nanodegree_-_part_4_-_data_pipelines_with_airflow.html#data-quality","text":"Q: What is data lineage ? Answer : The data lineage of a dataset describes the discrete steps involved in the creation, movement, and calculation of that dataset. Q: Why is data lineage important? Answer : Instilling Confidence: Being able to describe the data lineage of a particular dataset or analysis will build confidence in data consumers (engineers, analysts, data scientists, etc.) that our data pipeline is creating meaningful results using the correct datasets. Defining Metrics: Another major benefit of surfacing data lineage is that it allows everyone in the organization to agree on the definition of how a particular metric is calculated. Debugging: Data lineage helps data engineers track down the root of errors when they occur. If each step of the data movement and transformation process is well described, it's easy to find problems when they occur. Remarks : In general, data lineage has important implications for a business. Each department or business unit's success is tied to data and to the flow of data between departments. For e.g., sales departments rely on data to make sales forecasts, while at the same time the finance department would need to track sales and revenue. Each of these departments and roles depend on data, and knowing where to find the data. Data flow and data lineage tools enable data engineers and architects to track the flow of this large web of data. Q: Why use schedules in airflow? Answer : Pipeline schedules can reduce the amount of data that needs to be processed in a given run. It helps scope the job to only run the data for the time period since the data pipeline last ran. Using schedules to select only data relevant to the time period of the given pipeline execution can help improve the quality and accuracy of the analyses performed by our pipeline. Running pipelines on a schedule will decrease the time it takes the pipeline to run. An analysis of larger scope can leverage already-completed work . For example, if the aggregates for all months prior to now have already been done by a scheduled job, then we only need to perform the aggregation for the current month and add it to the existing totals. Q: Which factors should be considered in selecting a time period for scheduling? Answer : What is the size of data, on average, for a time period? If an entire years worth of data is only a few kb or mb, then perhaps its fine to load the entire dataset. If an hours worth of data is hundreds of mb or even in the gbs then likely you will need to schedule your pipeline more frequently. How frequently is data arriving, and how often does the analysis need to be performed? If our bikeshare company needs trip data every hour, that will be a driving factor in determining the schedule. Alternatively, if we have to load hundreds of thousands of tiny records, even if they don't add up to much in terms of mb or gb, the file access alone will slow down our analysis and we\u2019ll likely want to run it more often. What's the frequency on related datasets? A good rule of thumb is that the frequency of a pipeline\u2019s schedule should be determined by the dataset in our pipeline which requires the most frequent analysis. This isn\u2019t universally the case, but it's a good starting assumption. For example, if our trips data is updating every hour, but our bikeshare station table only updates once a quarter, we\u2019ll probably want to run our trip analysis every hour, and not once a quarter. Q: What is the scope of a data pipeline? Answer : It is the time delta between the current execution time and the end time of the last execution. Q: What is data partitioning ? Answer : It is the process of isolating data to be analyzed by one or more attributes, such as time, logical type, or data size. Remarks : It often leads to faster and more reliable pipelines. Q: Why should you use data partitioning ? Answer : Pipelines designed to work with partitioned data fail more gracefully . Smaller datasets, smaller time periods, and related concepts are easier to debug than big datasets, large time periods, and unrelated concepts. Partitioning makes rerunning failed tasks much simpler . It also enables easier redos of work, reducing cost and time. Remarks : Another great thing about Airflow is that if your data is partitioned appropriately, your tasks will naturally have fewer dependencies on each other. Because of this, Airflow will be able to parallelize execution of your DAGs to produce your results even faster. Q: What are four common types of data partitioning? Answer : Location Logical Size Time Q: What is logical partitioning ? Answer : The process of breaking conceptually related data into discrete groups for processing. Q: What is time partitioning ? Answer : It is the process of processing data based on a schedule or when it was created. Q: What is size partitioning ? Answer : It is the process of separating data for processing based on desired or required storage limits. Q: How would you set a requirement for ensuring that data arrives within a certain timeframe of a DAG starting? Answer : Using a service level agreement (SLA).","title":"Data Quality"},{"location":"knowledge/data_engineering_nanodegree_-_part_4_-_data_pipelines_with_airflow.html#module-3","text":"","title":"Module 3"},{"location":"knowledge/data_engineering_nanodegree_-_part_4_-_data_pipelines_with_airflow.html#production-data-pipelines","text":"Q: What are the most common types of user-created plugins? Answer : Operators and Hooks. Q: How to create a custom operator? Answer : Identify Operators that perform similar functions and can be consolidated Define a new Operator in the plugins folder Replace the original Operators with your new custom one, re-parameterize, and instantiate them. Q: What is Airflow contrib and why is it useful? Answer : It contains Operators and hooks for common data tools like Apache Spark and Cassandra, as well as vendor specific integrations for Amazon Web Services, Azure, and Google Cloud Platform can be found in Airflow contrib. Therefore you should always check it before building your own airflow plugins to see if what you need already exists. Remarks : If the functionality exists and its not quite what you want, that\u2019s a great opportunity to add that functionality through an open source contribution. Q: Which three rules should you follow when designing DAGs? Answer : DAG tasks should be designed such that they: are Atomic and have a single purpose Maximize parallelism Make failure states obvious Remarks : Every task in your dag should perform only one job. \u201cWrite programs that do one thing and do it well.\u201d - Ken Thompson\u2019s Unix Philosophy Q: What are the benefits of task boundaries ? Answer : Re-visitable: Task boundaries are useful for you if you revisit a pipeline you wrote after a 6 month absence. You'll have a much easier time understanding how it works and the lineage of the data if the boundaries between tasks are clear and well defined. This is true in the code itself, and within the Airflow UI. Tasks that do just one thing are often more easily parallelized . This parallelization can offer a significant speedup in the execution of our DAGs. Q: What are benefits of SubDAGs ? Answer : Decrease the amount of code we need to write and maintain to create a new DAG Easier to understand the high level goals of a DAG Bug fixes, speedups, and other enhancements can be made more quickly and distributed to all DAGs that use that SubDAG Q: What are drawbacks of using SubDAGs? Answer : Limit the visibility within the Airflow UI Abstraction makes understanding what the DAG is doing more difficult Encourages premature optimization Q: Which pipeline monitoring options does airflow provide? Answer : SLAs Emails and alerts Metrics Q: What's the purpose of SLAs in airflow? Answer : SLAs define the time by which a DAG must complete. For time-sensitive applications they are critical for developing trust amongst your pipeline customers and ensuring that data is delivered while it is still meaningful. Slipping SLAs can also be early indicators of performance problems , or a need to scale up the size of your Airflow cluster","title":"Production Data Pipelines"},{"location":"knowledge/data_engineering_nanodegree_-_part_4_-_data_pipelines_with_airflow.html#acronyms","text":"SLA : Service Level Agreement DAG : Directed Acyclic Graph","title":"Acronyms"}]}